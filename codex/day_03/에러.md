/home/ubuntu/miniconda/envs/vqa/lib/python3.11/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/ubuntu/miniconda/envs/vqa/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 [ 220/1095 1:21:30 < 5:27:11, 0.04 it/s, Epoch 1/5]
Epoch	Training Loss	Validation Loss
---------------------------------------------------------------------------
OutOfMemoryError                          Traceback (most recent call last)
Cell In[14], line 71
     61 training_args = TrainingArguments(**common_kwargs)
     63 trainer = Trainer(
     64     model=model,
     65     args=training_args,
   (...)     68     data_collator=fine_tune_collate_fn,
     69 )
---> 71 train_result = trainer.train()
     72 print("Training metrics:", getattr(train_result, "metrics", {}))
     74 try:

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:2325, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   2323         hf_hub_utils.enable_progress_bars()
   2324 else:
-> 2325     return inner_training_loop(
   2326         args=args,
   2327         resume_from_checkpoint=resume_from_checkpoint,
   2328         trial=trial,
   2329         ignore_keys_for_eval=ignore_keys_for_eval,
   2330     )

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:2790, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2787     self.control.should_training_stop = True
   2789 self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
-> 2790 self._maybe_log_save_evaluate(
   2791     tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=learning_rate
   2792 )
   2794 if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
   2795     if is_torch_xla_available():
   2796         # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:3221, in Trainer._maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)
   3219 metrics = None
   3220 if self.control.should_evaluate:
-> 3221     metrics = self._evaluate(trial, ignore_keys_for_eval)
   3222     is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
   3224     if self.args.save_strategy == SaveStrategy.BEST:

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:3170, in Trainer._evaluate(self, trial, ignore_keys_for_eval, skip_scheduler)
   3169 def _evaluate(self, trial, ignore_keys_for_eval, skip_scheduler=False):
-> 3170     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
   3171     self._report_to_hp_search(trial, self.state.global_step, metrics)
   3173     # Run delayed LR scheduler now that metrics are populated

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:4489, in Trainer.evaluate(self, eval_dataset, ignore_keys, metric_key_prefix)
   4486 start_time = time.time()
   4488 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-> 4489 output = eval_loop(
   4490     eval_dataloader,
   4491     description="Evaluation",
   4492     # No point gathering the predictions if there are no metrics, otherwise we defer to
   4493     # self.args.prediction_loss_only
   4494     prediction_loss_only=True if self.compute_metrics is None else None,
   4495     ignore_keys=ignore_keys,
   4496     metric_key_prefix=metric_key_prefix,
   4497 )
   4499 total_batch_size = self.args.eval_batch_size * self.args.world_size
   4500 if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:4685, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)
   4682         batch_size = observed_batch_size
   4684 # Prediction step
-> 4685 losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
   4686 main_input_name = getattr(self.model, "main_input_name", "input_ids")
   4687 inputs_decode = (
   4688     self._prepare_input(inputs[main_input_name]) if "inputs" in args.include_for_metrics else None
   4689 )

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:4902, in Trainer.prediction_step(self, model, inputs, prediction_loss_only, ignore_keys)
   4900 with self.compute_loss_context_manager():
   4901     num_items_in_batch = self._get_num_items_in_batch([inputs], self.args.device)
-> 4902     loss, outputs = self.compute_loss(
   4903         model, inputs, return_outputs=True, num_items_in_batch=num_items_in_batch
   4904     )
   4905 loss = loss.detach().mean()
   4907 if isinstance(outputs, dict):

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/trainer.py:4110, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)
   4108         kwargs["num_items_in_batch"] = num_items_in_batch
   4109     inputs = {**inputs, **kwargs}
-> 4110 outputs = model(**inputs)
   4111 # Save past state if it exists
   4112 # TODO: this needs to be fixed and made cleaner later.
   4113 if self.args.past_index >= 0:

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:819, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)
    818 def forward(*args, **kwargs):
--> 819     return model_forward(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:807, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)
    806 def __call__(self, *args, **kwargs):
--> 807     return convert_to_fp32(self.model_forward(*args, **kwargs))

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/amp/autocast_mode.py:43, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)
     40 @functools.wraps(func)
     41 def decorate_autocast(*args, **kwargs):
     42     with autocast_instance:
---> 43         return func(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:819, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)
    818 def forward(*args, **kwargs):
--> 819     return model_forward(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:807, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)
    806 def __call__(self, *args, **kwargs):
--> 807     return convert_to_fp32(self.model_forward(*args, **kwargs))

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/amp/autocast_mode.py:43, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)
     40 @functools.wraps(func)
     41 def decorate_autocast(*args, **kwargs):
     42     with autocast_instance:
---> 43         return func(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:819, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)
    818 def forward(*args, **kwargs):
--> 819     return model_forward(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/accelerate/utils/operations.py:807, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)
    806 def __call__(self, *args, **kwargs):
--> 807     return convert_to_fp32(self.model_forward(*args, **kwargs))

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/amp/autocast_mode.py:43, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)
     40 @functools.wraps(func)
     41 def decorate_autocast(*args, **kwargs):
     42     with autocast_instance:
---> 43         return func(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/peft/peft_model.py:1850, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)
   1848     with self._enable_peft_forward_hooks(**kwargs):
   1849         kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}
-> 1850         return self.base_model(
   1851             input_ids=input_ids,
   1852             attention_mask=attention_mask,
   1853             inputs_embeds=inputs_embeds,
   1854             labels=labels,
   1855             output_attentions=output_attentions,
   1856             output_hidden_states=output_hidden_states,
   1857             return_dict=return_dict,
   1858             **kwargs,
   1859         )
   1861 batch_size = _get_batch_size(input_ids, inputs_embeds)
   1862 if attention_mask is not None:
   1863     # concat prompt attention mask

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)
   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1552 else:
-> 1553     return self._call_impl(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)
   1557 # If we don't have any hooks, we want to skip the rest of the logic in
   1558 # this function, and just call forward.
   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1560         or _global_backward_pre_hooks or _global_backward_hooks
   1561         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1562     return forward_call(*args, **kwargs)
   1564 try:
   1565     result = None

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:222, in BaseTuner.forward(self, *args, **kwargs)
    221 def forward(self, *args: Any, **kwargs: Any):
--> 222     return self.model.forward(*args, **kwargs)

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/utils/generic.py:918, in can_return_tuple.<locals>.wrapper(self, *args, **kwargs)
    916 if return_dict_passed is not None:
    917     return_dict = return_dict_passed
--> 918 output = func(self, *args, **kwargs)
    919 if not return_dict and not isinstance(output, tuple):
    920     output = output.to_tuple()

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1503, in Qwen2_5_VLForConditionalGeneration.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)
   1501 loss = None
   1502 if labels is not None:
-> 1503     loss = self.loss_function(
   1504         logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs
   1505     )
   1507 return Qwen2_5_VLCausalLMOutputWithPast(
   1508     loss=loss,
   1509     logits=logits,
   (...)   1513     rope_deltas=outputs.rope_deltas,
   1514 )

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/loss/loss_utils.py:67, in ForCausalLMLoss(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)
     65 # Enable model parallelism
     66 shift_labels = shift_labels.to(logits.device)
---> 67 loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
     68 return loss

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/transformers/loss/loss_utils.py:36, in fixed_cross_entropy(source, target, num_items_in_batch, ignore_index, **kwargs)
     28 def fixed_cross_entropy(
     29     source: torch.Tensor,
     30     target: torch.Tensor,
   (...)     33     **kwargs,
     34 ) -> torch.Tensor:
     35     reduction = "sum" if num_items_in_batch is not None else "mean"
---> 36     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
     37     if reduction == "sum":
     38         # just in case users pass an int for num_items_in_batch, which could be the case for custom trainer
     39         if torch.is_tensor(num_items_in_batch):

File ~/miniconda/envs/vqa/lib/python3.11/site-packages/torch/nn/functional.py:3104, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3102 if size_average is not None or reduce is not None:
   3103     reduction = _Reduction.legacy_get_string(size_average, reduce)
-> 3104 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)

OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacity of 22.06 GiB of which 3.62 GiB is free. Including non-PyTorch memory, this process has 18.43 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 4.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)