{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb06f314",
   "metadata": {},
   "source": [
    "# QLoRA íŒŒì¸íŠœë‹ ì›Œí¬í”Œë¡œìš° (10ë‹¨ê³„)\n",
    "- ì‘ì„±ì¼: 2025-10-23 00:46:20\n",
    "- ëŒ€ìƒ: Colab/Kaggle T4(1ì¥) ë˜ëŠ” AWS g4dn.xlarge(T4Ã—1) / ë©€í‹°-GPUëŠ” DDP\n",
    "- í”„ë ˆì„ì›Œí¬: `unsloth + transformers + peft + accelerate + bitsandbytes`\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **ëŒ€íšŒ í™˜ê²½ì—ì„œ ë¹ ë¥´ê²Œ QLoRA íŒŒì¸íŠœë‹**ì„ ì§„í–‰í•˜ê¸° ìœ„í•œ ìµœì†ŒÂ·í•„ìˆ˜ ë‹¨ê³„ë¥¼ 10ì…€ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "ê° ì…€ì€ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ í•™ìŠµ â†’ ê²€ì¦ â†’ ì¶”ë¡ ê¹Œì§€ ì™„ë£Œë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b552094",
   "metadata": {},
   "source": [
    "## 1) í™˜ê²½/ì„¤ì • â†’ ì‹œë“œÂ·ê²½ë¡œÂ·í•˜ë“œì›¨ì–´(T4Ã—2, 4bit, bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# (ì„ íƒ) ëŸ°íƒ€ì„ì— ì„¤ì¹˜ê°€ í•„ìš”í•œ ê²½ìš° ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "# pip install -q \"unsloth[cu121]\" transformers peft bitsandbytes accelerate datasets trl rouge-score evaluate\n",
    "# pip install -q scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ì„¤ì •\n",
    "import os, random, numpy as np, torch\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "PROJ = './qlora_run'\n",
    "os.makedirs(PROJ, exist_ok=True)\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print('GPU count:', n_gpu)\n",
    "if n_gpu:\n",
    "    for i in range(n_gpu):\n",
    "        print(f'CUDA:{i}', torch.cuda.get_device_name(i))\n",
    "BF16 = torch.cuda.is_bf16_supported()\n",
    "print('BF16 supported:', BF16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419e964",
   "metadata": {},
   "source": [
    "## 2) ë°ì´í„° ì¸ë±ì‹± â†’ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—…, ì¤‘ë³µ/ê¹¨ì§„ ìƒ˜í”Œ ìŠ¤í‚µ\n",
    "- ì…ë ¥ í¬ë§· ì˜ˆì‹œ: `train.jsonl` (í•„ìˆ˜: `instruction`, `output`; ì„ íƒ: `source`, `lang`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fe151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, hashlib\n",
    "\n",
    "TRAIN_JSONL = 'train.jsonl'  # ê²½ë¡œë¥¼ ìƒí™©ì— ë§ê²Œ ìˆ˜ì •\n",
    "\n",
    "def stream_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "seen = set(); data = []\n",
    "for ex in stream_jsonl(TRAIN_JSONL):\n",
    "    if not ex.get('instruction') or not ex.get('output'):\n",
    "        continue\n",
    "    key = hashlib.md5((normalize_text(ex['instruction']) + normalize_text(ex['output'])).encode()).hexdigest()\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key); data.append(ex)\n",
    "print('Loaded samples:', len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493456d8",
   "metadata": {},
   "source": [
    "## 3) EDA â†’ ê¸¸ì´ ë¶„í¬/ë²„í‚·(ì¸µí™”ìš©), ìƒ˜í”Œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "BASE_MODEL = 'unsloth/phi-3-mini-4k-instruct-bnb-4bit'  # ë˜ëŠ” llama-3-8b-instruct 4bit ê°€ìš© í™˜ê²½\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "PROMPT_SYS = 'You are a helpful assistant.'\n",
    "def count_tokens(ex):\n",
    "    s = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{ex['instruction']}\\n[ASSISTANT]{ex['output']}</s>\"\n",
    "    return len(tok(s, add_special_tokens=False)['input_ids'])\n",
    "\n",
    "lens = [count_tokens(x) for x in data]\n",
    "pcts = np.percentile(lens, [50,90,95,99]).tolist()\n",
    "print('Token length p50/90/95/99:', pcts, 'max:', max(lens) if lens else None)\n",
    "\n",
    "def bucketize(n):\n",
    "    # [<=512:0, <=1024:1, <=1536:2, <=2048:3, >2048:4]\n",
    "    if n <= 512: return 0\n",
    "    if n <= 1024: return 1\n",
    "    if n <= 1536: return 2\n",
    "    if n <= 2048: return 3\n",
    "    return 4\n",
    "\n",
    "buckets = [bucketize(l) for l in lens]\n",
    "from collections import Counter\n",
    "print('Length bucket dist:', Counter(buckets))\n",
    "\n",
    "# (ì„ íƒ) íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(lens, bins=30)\n",
    "plt.title('Token length histogram')\n",
    "plt.xlabel('tokens'); plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d3c30",
   "metadata": {},
   "source": [
    "## 4) ì „ì²˜ë¦¬/í¬ë§· â†’ í…œí”Œë¦¿ í†µì¼, ë¼ë²¨ ë§ˆìŠ¤í‚¹(-100), ì‹œí€€ìŠ¤ íŒ¨í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024  # ì—¬ìœ  ì‹œ 1536/2048\n",
    "\n",
    "def build_ids_and_labels(ex):\n",
    "    user = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{ex['instruction']}\\n[ASSISTANT]\"\n",
    "    ans  = ex['output'] + tok.eos_token\n",
    "    u = tok(user, add_special_tokens=False)\n",
    "    a = tok(ans, add_special_tokens=False)\n",
    "    input_ids = u['input_ids'] + a['input_ids']\n",
    "    labels    = [-100]*len(u['input_ids']) + a['input_ids']\n",
    "    attn_mask = [1]*len(input_ids)\n",
    "    return {'input_ids': input_ids, 'labels': labels, 'attention_mask': attn_mask}\n",
    "\n",
    "proc = [build_ids_and_labels(x) for x in data]\n",
    "\n",
    "def pack(samples, max_len=MAX_LEN):\n",
    "    out=[]\n",
    "    cur={'input_ids':[], 'labels':[], 'attention_mask':[]}\n",
    "    for s in samples:\n",
    "        L = len(s['input_ids'])\n",
    "        if L > max_len:  # ë„ˆë¬´ ê¸´ ìƒ˜í”Œì€ ì˜ë¼ë‚´ê¸°\n",
    "            s={'input_ids': s['input_ids'][:max_len],\n",
    "               'labels': s['labels'][:max_len],\n",
    "               'attention_mask': s['attention_mask'][:max_len]}\n",
    "        if len(cur['input_ids']) + len(s['input_ids']) > max_len and cur['input_ids']:\n",
    "            out.append(cur)\n",
    "            cur={'input_ids':[], 'labels':[], 'attention_mask':[]}\n",
    "        for k in cur:\n",
    "            cur[k] += s[k]\n",
    "    if cur['input_ids']:\n",
    "        out.append(cur)\n",
    "    return out\n",
    "\n",
    "print('Example packed length check: before', len(proc), 'â†’ after (by bucket later)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8531cbf",
   "metadata": {},
   "source": [
    "## 5) ì»¤ìŠ¤í…€ Dataset/Dataloader êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xs): self.xs = xs\n",
    "    def __len__(self): return len(self.xs)\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v) for k,v in self.xs[i].items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaf479",
   "metadata": {},
   "source": [
    "## 6) ëª¨ë¸ ë¹Œë” â†’ Unsloth 4bit ë¡œë“œ + LoRA íƒ€ê¹ƒ ëª¨ë“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "base, tok = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1, bias='none',\n",
    "    target_modules=['q_proj','v_proj','k_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "print('Model ready (LoRA attached).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb54a2",
   "metadata": {},
   "source": [
    "## 7) í•™ìŠµ ìœ í‹¸ â†’ ì†ì‹¤/ì§€í‘œ, bf16, EarlyStopping, Cosine ìŠ¤ì¼€ì¤„ëŸ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{PROJ}/ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps', eval_steps=200,\n",
    "    save_strategy='steps', save_steps=200, save_total_limit=3,\n",
    "    bf16=BF16, fp16=not BF16,\n",
    "    gradient_checkpointing=True, max_grad_norm=1.0,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "print('TrainingArguments ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138ecca",
   "metadata": {},
   "source": [
    "## 8) Stratified K-Fold í•™ìŠµ(ê¸¸ì´ ë²„í‚· ê¸°ì¤€ 3~5-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np, random\n",
    "\n",
    "X = np.arange(len(proc)); y = np.array(buckets)\n",
    "SK = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_paths = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(SK.split(X, y), 1):\n",
    "    tr = [proc[i] for i in tr_idx]; va = [proc[i] for i in va_idx]\n",
    "    # ê¸¸ì´ ë²„í‚·ë³„ë¡œ ì„ì€ ë’¤ íŒ¨í‚¹(ê°„ë‹¨ êµ¬í˜„)\n",
    "    def by_bucket(items, idxs):\n",
    "        return [items[i] for i in idxs]\n",
    "    tr_pack = pack(tr, MAX_LEN); va_pack = pack(va, MAX_LEN)\n",
    "    train_ds = ListDataset(tr_pack); val_ds = ListDataset(va_pack)\n",
    "\n",
    "    fold_out = f\"{PROJ}/fold{fold}\"\n",
    "    os.makedirs(fold_out, exist_ok=True)\n",
    "\n",
    "    fold_args = args.clone()\n",
    "    fold_args.output_dir = fold_out\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=fold_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tok,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # ì£¼ì˜: ì‹¤ì œ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. í•„ìš”ì‹œ ì£¼ì„ í•´ì œ.\n",
    "    # trainer.train()\n",
    "    # trainer.save_model(f\"{fold_out}/adapter\")\n",
    "    \n",
    "    fold_paths.append(fold_out)\n",
    "\n",
    "print('Fold output paths:', fold_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910596e",
   "metadata": {},
   "source": [
    "## 9) í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€(ìµœëŒ€ 2,000ë¬¸í•­) â†’ ë¦¬í¬íŠ¸/ìƒ˜í”Œ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "TEST_JSONL = 'test.jsonl'  # ê²½ë¡œ ìˆ˜ì •\n",
    "\n",
    "def stream_test(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def generate_answer(prompt, max_new_tokens=256):\n",
    "    s = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{prompt}\\n[ASSISTANT]\"\n",
    "    ids = tok(s, return_tensors='pt')\n",
    "    device = next(model.parameters()).device\n",
    "    ids = {k:v.to(device) for k,v in ids.items()}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**ids, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split('[ASSISTANT]')[-1].strip()\n",
    "\n",
    "pred_rows=[]\n",
    "# ì‹¤ì œ ìƒì„±ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”ì‹œ ì œí•œí•˜ì„¸ìš”.\n",
    "# for i, ex in enumerate(stream_test(TEST_JSONL)):\n",
    "#     if i>=2000: break\n",
    "#     pred = generate_answer(ex['instruction'])\n",
    "#     pred_rows.append({'instruction': ex['instruction'], 'prediction': pred})\n",
    "\n",
    "df = pd.DataFrame(pred_rows)\n",
    "csv_path = f\"{PROJ}/preds.csv\"\n",
    "if len(df):\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print('Saved:', csv_path)\n",
    "else:\n",
    "    print('ìƒì„± ì˜ˆì‹œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìƒë‹¨ ë£¨í”„ ì£¼ì„ì„ í•´ì œ í›„ ì‹¤í–‰í•˜ì„¸ìš”.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b9a5d",
   "metadata": {},
   "source": [
    "## 10) ê²°ê³¼ ì·¨í•©/ë² ìŠ¤íŠ¸ ì €ì¥ + (ì˜µì…˜) LoRA ë³‘í•©(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ì–´ëŒ‘í„°ë§Œ ì‚¬ìš©í•  ê²½ìš°: foldë³„ adapter ë””ë ‰í† ë¦¬ ë³´ê´€\n",
    "# (ì˜µì…˜) ë³‘í•©í•˜ì—¬ ë‹¨ì¼ ì¶”ë¡  ê°€ì¤‘ì¹˜ë¡œ ì €ì¥\n",
    "# ì£¼ì˜: ë³‘í•©ì€ FP16 ë©”ëª¨ë¦¬ í•„ìš”\n",
    "\n",
    "# merged = FastLanguageModel.merge_and_unload(model)\n",
    "# merged.save_pretrained(f\"{PROJ}/merged_full_fp16\")\n",
    "# tok.save_pretrained(f\"{PROJ}/merged_full_fp16\")\n",
    "print('Done. í•„ìš” ì‹œ ë³‘í•© ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•˜ì„¸ìš”.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0e913",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ… ì˜¤ë²„í”¼íŒ…/ì–¸ë”í”¼íŒ… ë¹ ë¥¸ ì²˜ë°©\n",
    "- **ì˜¤ë²„í”¼íŒ…**: `lora_dropout 0.1â†’0.3`, `r 16â†’8`, í•™ìŠµ ìŠ¤í…â†“, í…œí”Œë¦¿ í†µì¼, ì¤‘ë³µ ì œê±°\n",
    "- **ì–¸ë”í”¼íŒ…**: `r 16â†’32`, target_modules í™•ëŒ€, stepsâ†‘, ë°ì´í„° í´ë¦°ì—…/ë³´ê°•\n",
    "- **OOM**: `MAX_LENâ†“(1024â†’768)`, grad_accumâ†‘, rankâ†“, íŒ¨í‚¹ ìœ ì§€\n",
    "\n",
    "### ğŸ”§ ë©€í‹° GPU(T4Ã—2)\n",
    "```bash\n",
    "accelerate config   # distributed_type=NO(DDP) ë˜ëŠ” AUTO, mixed_precision=bf16\n",
    "accelerate launch your_train_script.py\n",
    "```\n",
    "\n",
    "### ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬\n",
    "- `save_total_limit=3`ë¡œ ìš©ëŸ‰ ì ˆì•½\n",
    "- **LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥** ê¶Œì¥ (ë°°í¬/ì œì¶œ ì‹œ ìš©ëŸ‰â†“)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
