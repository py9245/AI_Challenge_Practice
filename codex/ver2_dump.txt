
# Cell 2
import os
import platform
import torch

print(f"Python: {platform.python_version()}")
print(f"Torch: {torch.__version__}")
print(f"CUDA devices: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    for idx in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(idx)
        print(f"  GPU {idx}: {props.name} ({props.total_memory / 1024 ** 3:.1f} GB)")
else:
    print("CUDA not available; switch Kaggle accelerator to GPU (T4x2).")

!nvidia-smi
!df -h /kaggle/working

# Cell 4
%%capture
!pip install -q -U transformers accelerate einops tiktoken huggingface_hub qwen-vl-utils peft bitsandbytes

# Cell 6
import json
import math
import os
import random
import re
from collections import Counter
from pathlib import Path

import numpy as np
import pandas as pd
from IPython.display import display
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

import matplotlib.pyplot as plt
import torch
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoProcessor

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

plt.rcParams["figure.figsize"] = (6, 6)
plt.rcParams["axes.grid"] = False
pd.set_option("display.max_columns", None)

os.environ["TOKENIZERS_PARALLELISM"] = "false"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

INPUT_DIR = Path("/kaggle/input")
WORK_DIR = Path("/kaggle/working")
ARTIFACT_DIR = WORK_DIR / "artifacts"
ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)

# ==============================
# Hugging Face Token Load
# ==============================
from kaggle_secrets import UserSecretsClient

user_secrets = UserSecretsClient()
HF_TOKEN = user_secrets.get_secret("HF_TOKEN")

if HF_TOKEN:
    os.environ["HF_TOKEN"] = HF_TOKEN
    print("✅ HF_TOKEN successfully loaded!")
else:
    print("⚠️ HF_TOKEN not found. Add it via Kaggle Secrets before downloading gated models.")

# Cell 8
import zipfile

data_keyword = "2025-ssafy-14"
archive_candidates = sorted(INPUT_DIR.rglob(f"{data_keyword}.zip"))
if archive_candidates:
    archive_path = archive_candidates[0]
    DATA_ROOT = WORK_DIR / data_keyword
    if not (DATA_ROOT / "train.csv").exists():
        print(f"Extracting {archive_path} -> {DATA_ROOT}")
        DATA_ROOT.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(archive_path, "r") as zf:
            zf.extractall(DATA_ROOT)
    else:
        print(f"Using existing extracted dataset at {DATA_ROOT}")
else:
    print("Zip archive not located; searching for raw CSV files in /kaggle/input ...")
    csv_candidates = sorted(INPUT_DIR.rglob("train.csv"))
    if not csv_candidates:
        raise FileNotFoundError("Unable to locate train.csv under /kaggle/input")
    DATA_ROOT = csv_candidates[0].parent
    print(f"Using dataset directory detected at {DATA_ROOT}")

print(f"DATA_ROOT resolved to: {DATA_ROOT.resolve()}")
print("Contents:")
for child in sorted(DATA_ROOT.iterdir()):
    print(f"- {child.name}")

# Cell 10
train_df = pd.read_csv(DATA_ROOT / "train.csv")
test_df = pd.read_csv(DATA_ROOT / "test.csv")
sample_submission = pd.read_csv(DATA_ROOT / "sample_submission.csv")

train_df["answer"] = train_df["answer"].str.lower()
train_df["image_path"] = train_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))
test_df["image_path"] = test_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))

print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(f"Sample submission shape: {sample_submission.shape}")

display(train_df.head())

# Cell 12
label_distribution = train_df["answer"].value_counts().sort_index()
print("Label distribution (train):")
display(label_distribution.to_frame(name="count"))

duplicate_ids = train_df["id"].duplicated().sum()
print(f"Duplicate train IDs: {duplicate_ids}")

null_summary = train_df.isna().sum()
print("Null counts (train):")
display(null_summary[null_summary > 0])

sample_paths = train_df["image_path"].sample(n=min(256, len(train_df)), random_state=SEED)
widths, heights = [], []
for path in sample_paths:
    with Image.open(path) as img:
        w, h = img.size
    widths.append(w)
    heights.append(h)

print(
    f"Sample width px -> mean: {np.mean(widths):.1f}, min: {min(widths)}, max: {max(widths)}"
)
print(
    f"Sample height px -> mean: {np.mean(heights):.1f}, min: {min(heights)}, max: {max(heights)}"
)

# Cell 14
num_examples = 4
fig, axes = plt.subplots(1, num_examples, figsize=(4 * num_examples, 4))
for ax, (_, row) in zip(axes, train_df.sample(num_examples, random_state=SEED).iterrows()):
    with Image.open(row["image_path"]) as img:
        ax.imshow(img)
    ax.axis("off")
    title_lines = [
        row["id"],
        row["question"][:40] + ("..." if len(row["question"]) > 40 else ""),
        f"Answer: {row['answer'].upper()}"
    ]
    ax.set_title("".join(title_lines), fontsize=9)
plt.tight_layout()

sample_row = train_df.sample(1, random_state=SEED).iloc[0]
print("Sample question:", sample_row["question"])
for key in ["a", "b", "c", "d"]:
    print(f"{key.upper()}: {sample_row[key]}")

# Cell 16
train_split, val_split = train_test_split(
    train_df,
    test_size=0.1,
    stratify=train_df["answer"],
    random_state=SEED,
)

train_split = train_split.reset_index(drop=True)
val_split = val_split.reset_index(drop=True)

print(f"Training rows: {len(train_split)}")
print(f"Validation rows: {len(val_split)}")
print("Validation label balance:")
display(val_split["answer"].value_counts().sort_index())

# Cell 18
SYSTEM_PROMPT = (
    "You are a Korean visual question answering assistant. "
    "Inspect the provided image carefully and choose the correct option among a, b, c, d. "
    "Respond strictly using a JSON object like {\"answer\": \"a\"}. No explanations, no additional text."
)

CHOICE_KEYS = ["a", "b", "c", "d"]

def create_user_block(row):
    option_lines = "".join(f"{key.upper()}. {row[key]}" for key in CHOICE_KEYS)
    return (f"질문:{row['question']}"f"선택지:{option_lines}"
        "응답 형식: JSON 예시 {\"answer\": \"a\"} (소문자, 따옴표 유지, 설명 금지)."
    )

def build_messages(row):
    return [
        {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": create_user_block(row)},
            ],
        },
    ]


def load_image(path: str) -> Image.Image:
    with Image.open(path) as img:
        return img.convert("RGB")

print("Prompt helpers ready.")

# Cell 20
import importlib.util
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

if HF_TOKEN:
    login(token=HF_TOKEN, add_to_git_credential=False)
else:
    print("Proceeding without HF login; ensure the model is public for your account.")

processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_ID,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config,
)

model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

candidate_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"]
found_modules = set()
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear) and any(token in name for token in candidate_modules):
        found_modules.add(name.split(".")[-1])
if not found_modules:
    found_modules = {"q_proj", "k_proj", "v_proj", "o_proj"}

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=sorted(found_modules),
)

model = get_peft_model(model, lora_config)
model.config.use_cache = False

if model.generation_config.pad_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.pad_token_id = processor.tokenizer.pad_token_id
if model.generation_config.eos_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.eos_token_id = processor.tokenizer.eos_token_id

if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "right"

print("Model loaded with 4-bit quantization and LoRA adapters. Trainable parameters:")
model.print_trainable_parameters()

# Cell 22
from torch.utils.data import Dataset
import json
import torch

MAX_SEQUENCE_LENGTH = 1024

class VqaFineTuneDataset(Dataset):
    def __init__(self, dataframe):
        self.df = dataframe.reset_index(drop=True)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {
            "image_path": row["image_path"],
            "messages": build_messages(row),
            "answer": str(row["answer"]).lower(),
        }

def _build_conversation(sample):
    # 정답 JSON까지 포함한 대화(학습 타깃)
    conversation = sample["messages"] + [
        {
            "role": "assistant",
            "content": [{"type": "text", "text": json.dumps({"answer": sample["answer"]})}],
        }
    ]
    # (1) 프롬프트만
    prompt_text = processor.apply_chat_template(
        sample["messages"], tokenize=False, add_generation_prompt=True
    )
    # (2) 프롬프트 + 정답JSON
    chat_text = processor.apply_chat_template(
        conversation, tokenize=False, add_generation_prompt=False
    )
    return prompt_text, chat_text, sample["image_path"]

# ★ 디코더 전용 모델은 left padding 필수
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

def fine_tune_collate_fn(batch):
    images, prompts, full_texts = [], [], []

    for sample in batch:
        prompt_text, chat_text, image_path = _build_conversation(sample)
        images.append(load_image(image_path))
        prompts.append(prompt_text)
        full_texts.append(chat_text)

    # 🔹 프롬프트를 "이미지 포함"으로 인코딩 → 실제 프롬프트 길이(비전 토큰 포함) 획득
    enc_prompt = processor(
        images=images,
        text=prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    # 🔹 프롬프트+정답 전체 인코딩
    enc_full = processor(
        images=images,
        text=full_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )

    labels = enc_full["input_ids"].clone()
    # 패딩 토큰은 손실에서 제외
    labels[enc_full["attention_mask"] == 0] = -100

    # ✅ 패딩 방식과 무관하게 "프롬프트 구간"만 정확히 마스킹
    for i in range(labels.size(0)):
        full_mask   = enc_full["attention_mask"][i].bool()          # 전체에서 non-pad 위치
        prompt_len  = int(enc_prompt["attention_mask"][i].sum().item())  # 프롬프트의 non-pad 길이
        nonpad_idx  = torch.nonzero(full_mask, as_tuple=False).squeeze(-1)
        prompt_idx  = nonpad_idx[:prompt_len]   # non-pad 중 앞쪽 prompt_len개가 '프롬프트 토큰'
        labels[i, prompt_idx] = -100            # 프롬프트 구간 손실 제외

    enc_full["labels"] = labels
    return enc_full

train_dataset = VqaFineTuneDataset(train_split)
if len(val_split) <= 128:
    val_eval_df = val_split.copy().reset_index(drop=True)
else:
    val_eval_df = val_split.sample(n=128, random_state=SEED).reset_index(drop=True)
val_dataset = VqaFineTuneDataset(val_eval_df)
print(f"Train dataset size: {len(train_dataset)} | Eval dataset size: {len(val_dataset)}")

# Cell 24
# 12번: TrainingArguments 호환 패치 + Trainer 실행 (키 보존/안정화 포함)
import inspect
from transformers import TrainingArguments, Trainer

# 디코더 전용 모델은 left-padding 권장
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

OUTPUT_DIR = WORK_DIR / "qwen25vl_lora"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

EPOCHS = 1
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 1e-4

# 공통 인자 (remove_unused_columns=False 반드시!)
ta_kwargs = dict(
    output_dir=str(OUTPUT_DIR),
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    logging_steps=20,
    save_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available(),
    optim="paged_adamw_8bit",
    dataloader_num_workers=0,        # ← 워커 0: 에러 메시지 삼킴 방지
    max_grad_norm=1.0,
    remove_unused_columns=False,     # ← Trainer가 'image_path/messages'를 지우지 않도록
)

# ---- 호환 처리: evaluation_strategy / eval_strategy 중 있는 것만 사용 ----
sig = inspect.signature(TrainingArguments.__init__)
if "evaluation_strategy" in sig.parameters:
    ta_kwargs["evaluation_strategy"] = "epoch"
elif "eval_strategy" in sig.parameters:
    ta_kwargs["eval_strategy"] = "epoch"
# 둘 다 없는 버전이면 자동평가 건너뛰고 아래에서 수동 evaluate()

training_args = TrainingArguments(**ta_kwargs)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,   # 11번에서 만든 VqaFineTuneDataset
    eval_dataset=val_dataset,      # 11번에서 만든 VqaFineTuneDataset(샘플 128)
    data_collator=fine_tune_collate_fn,  # 11번의 collate_fn 사용
)

train_result = trainer.train()
print("Training metrics:", getattr(train_result, "metrics", {}))

# 자동 평가 기능이 없는 버전 대비: 수동 평가 시도
try:
    eval_metrics = trainer.evaluate(eval_dataset=val_dataset)
    print("Eval metrics:", eval_metrics)
except Exception as e:
    print(f"(info) 자동 평가 옵션 미지원 또는 건너뜀: {e}")

adapter_path = OUTPUT_DIR / "lora_adapter"
model.save_pretrained(adapter_path)                 # LoRA/PEFT 모델이면 어댑터 저장
processor.save_pretrained(OUTPUT_DIR / "processor")
print(f"LoRA fine-tuning complete. Adapter weights saved to {adapter_path}")

# Cell 26
LETTER_JSON_PATTERN = re.compile(r'"answer"\s*:\s*"([abcd])"')
LETTER_FALLBACK_PATTERN = re.compile(r"([abcd])")

PRED_BATCH_SIZE = 2
MAX_NEW_TOKENS = 24

def decode_answer_text(text: str) -> str:
    lowered = text.lower()
    match = LETTER_JSON_PATTERN.search(lowered)
    if match:
        return match.group(1)
    match = LETTER_FALLBACK_PATTERN.search(lowered)
    if match:
        return match.group(1)
    return "a"


def run_inference(records, batch_size: int = PRED_BATCH_SIZE, keep_raw: bool = False):
    outputs = []
    raw_outputs = [] if keep_raw else None
    for start in tqdm(range(0, len(records), batch_size)):
        batch = records[start : start + batch_size]
        prompts = []
        images = []
        for row in batch:
            prompts.append(
                processor.apply_chat_template(
                    build_messages(row),
                    tokenize=False,
                    add_generation_prompt=True,
                )
            )
            images.append(load_image(row["image_path"]))
        inputs = processor(
            text=prompts,
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        with torch.inference_mode():
            generated = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.0,
                do_sample=False,
            )
        decoded = processor.batch_decode(generated, skip_special_tokens=True)
        for response in decoded:
            answer = decode_answer_text(response)
            outputs.append(answer)
            if keep_raw:
                raw_outputs.append(response)
    if keep_raw:
        return outputs, raw_outputs
    return outputs

print("Inference helpers initialised.")

# Cell 28
# 12. Validation Run (OOM-safe, Kaggle T4 대응)
import gc
from tqdm.auto import tqdm
import torch

# Qwen2.5-VL 은 decoder-only 구조 → 반드시 left padding
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

# 너무 크게 돌리면 T4(16GB)에서 바로 죽음 → 샘플 수 줄이고, 배치=1로 순차 처리
VALIDATION_LIMIT = min(32, len(val_split))  # 기존 128 -> 32 (필요시 16/24로 더 줄여도 됨)
validation_subset = val_split.sample(VALIDATION_LIMIT, random_state=SEED).reset_index(drop=True)
validation_records = validation_subset.to_dict("records")

val_preds, raw_responses = [], []

def _infer_one(row):
    # 이미지/프롬프트 준비
    img_path = row["image_path"] if "image_path" in row else row["image"]  # 노트북 컬럼명에 맞춰 자동 대응
    image = load_image(img_path)
    messages = build_messages(row)
    chat_text = processor.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    # 토크나이저+이미지 인코딩 (배치=1)
    inputs = processor(
        images=[image],
        text=[chat_text],
        return_tensors="pt",
        padding=True,
    ).to(model.device)

    # 실제 생성: 배치=1, no temperature, 짧게, use_cache=False 로 VRAM 절감
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=8,        # 너무 길면 메모리 팍 늘어남
            do_sample=False,         # temperature 사용 안 함 (경고 제거)
            use_cache=False          # 캐시 끄면 느리지만 VRAM 사용량↓
        )
    text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]

    # 정리
    del inputs, output_ids, image
    gc.collect(); torch.cuda.empty_cache()
    return text

for row in tqdm(validation_records, total=len(validation_records)):
    out_text = _infer_one(row)
    raw_responses.append(out_text)

    # JSON 파싱 (당신 노트북의 파서 함수명이 다르면 바꿔주세요)
    # 예: pred = parse_json_answer(out_text, choice_keys=CHOICE_KEYS, default="a")
    try:
        m = re.search(r'{"\s*answer"\s*:\s*"(a|b|c|d)"}', out_text.strip(), flags=re.I)
        pred = m.group(1).lower() if m else "a"
    except Exception:
        pred = "a"
    val_preds.append(pred)

validation_subset = validation_subset.copy()
validation_subset["prediction"] = val_preds
validation_subset["raw_response"] = raw_responses
validation_subset["is_correct"] = (validation_subset["answer"].str.lower() == validation_subset["prediction"])

val_accuracy = validation_subset["is_correct"].mean()
print(f"✅ Validation accuracy on {len(validation_subset)} samples: {val_accuracy:.4f}")

display(validation_subset.head())
display(
    validation_subset[validation_subset["is_correct"] == False][
        ["id", "answer", "prediction", "raw_response"]
    ].head()
)

# Cell 30
test_records = test_df.to_dict("records")

test_predictions = run_inference(test_records)
submission = pd.DataFrame({
    "id": test_df["id"],
    "answer": test_predictions,
})

submission_path = WORK_DIR / "submission.csv"
submission.to_csv(submission_path, index=False)
print(f"Saved submission to {submission_path}")

display(submission.head())
