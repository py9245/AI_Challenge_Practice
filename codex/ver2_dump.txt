
# Cell 2
import os
import platform
import torch

print(f"Python: {platform.python_version()}")
print(f"Torch: {torch.__version__}")
print(f"CUDA devices: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    for idx in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(idx)
        print(f"  GPU {idx}: {props.name} ({props.total_memory / 1024 ** 3:.1f} GB)")
else:
    print("CUDA not available; switch Kaggle accelerator to GPU (T4x2).")

!nvidia-smi
!df -h /kaggle/working

# Cell 4
%%capture
!pip install -q -U transformers accelerate einops tiktoken huggingface_hub qwen-vl-utils peft bitsandbytes

# Cell 6
import json
import math
import os
import random
import re
from collections import Counter
from pathlib import Path

import numpy as np
import pandas as pd
from IPython.display import display
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

import matplotlib.pyplot as plt
import torch
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoProcessor

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

plt.rcParams["figure.figsize"] = (6, 6)
plt.rcParams["axes.grid"] = False
pd.set_option("display.max_columns", None)

os.environ["TOKENIZERS_PARALLELISM"] = "false"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

INPUT_DIR = Path("/kaggle/input")
WORK_DIR = Path("/kaggle/working")
ARTIFACT_DIR = WORK_DIR / "artifacts"
ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)

# ==============================
# Hugging Face Token Load
# ==============================
from kaggle_secrets import UserSecretsClient

user_secrets = UserSecretsClient()
HF_TOKEN = user_secrets.get_secret("HF_TOKEN")

if HF_TOKEN:
    os.environ["HF_TOKEN"] = HF_TOKEN
    print("âœ… HF_TOKEN successfully loaded!")
else:
    print("âš ï¸ HF_TOKEN not found. Add it via Kaggle Secrets before downloading gated models.")

# Cell 8
import zipfile

data_keyword = "2025-ssafy-14"
archive_candidates = sorted(INPUT_DIR.rglob(f"{data_keyword}.zip"))
if archive_candidates:
    archive_path = archive_candidates[0]
    DATA_ROOT = WORK_DIR / data_keyword
    if not (DATA_ROOT / "train.csv").exists():
        print(f"Extracting {archive_path} -> {DATA_ROOT}")
        DATA_ROOT.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(archive_path, "r") as zf:
            zf.extractall(DATA_ROOT)
    else:
        print(f"Using existing extracted dataset at {DATA_ROOT}")
else:
    print("Zip archive not located; searching for raw CSV files in /kaggle/input ...")
    csv_candidates = sorted(INPUT_DIR.rglob("train.csv"))
    if not csv_candidates:
        raise FileNotFoundError("Unable to locate train.csv under /kaggle/input")
    DATA_ROOT = csv_candidates[0].parent
    print(f"Using dataset directory detected at {DATA_ROOT}")

print(f"DATA_ROOT resolved to: {DATA_ROOT.resolve()}")
print("Contents:")
for child in sorted(DATA_ROOT.iterdir()):
    print(f"- {child.name}")

# Cell 10
train_df = pd.read_csv(DATA_ROOT / "train.csv")
test_df = pd.read_csv(DATA_ROOT / "test.csv")
sample_submission = pd.read_csv(DATA_ROOT / "sample_submission.csv")

train_df["answer"] = train_df["answer"].str.lower()
train_df["image_path"] = train_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))
test_df["image_path"] = test_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))

print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(f"Sample submission shape: {sample_submission.shape}")

display(train_df.head())

# Cell 12
label_distribution = train_df["answer"].value_counts().sort_index()
print("Label distribution (train):")
display(label_distribution.to_frame(name="count"))

duplicate_ids = train_df["id"].duplicated().sum()
print(f"Duplicate train IDs: {duplicate_ids}")

null_summary = train_df.isna().sum()
print("Null counts (train):")
display(null_summary[null_summary > 0])

sample_paths = train_df["image_path"].sample(n=min(256, len(train_df)), random_state=SEED)
widths, heights = [], []
for path in sample_paths:
    with Image.open(path) as img:
        w, h = img.size
    widths.append(w)
    heights.append(h)

print(
    f"Sample width px -> mean: {np.mean(widths):.1f}, min: {min(widths)}, max: {max(widths)}"
)
print(
    f"Sample height px -> mean: {np.mean(heights):.1f}, min: {min(heights)}, max: {max(heights)}"
)

# Cell 14
num_examples = 4
fig, axes = plt.subplots(1, num_examples, figsize=(4 * num_examples, 4))
for ax, (_, row) in zip(axes, train_df.sample(num_examples, random_state=SEED).iterrows()):
    with Image.open(row["image_path"]) as img:
        ax.imshow(img)
    ax.axis("off")
    title_lines = [
        row["id"],
        row["question"][:40] + ("..." if len(row["question"]) > 40 else ""),
        f"Answer: {row['answer'].upper()}"
    ]
    ax.set_title("".join(title_lines), fontsize=9)
plt.tight_layout()

sample_row = train_df.sample(1, random_state=SEED).iloc[0]
print("Sample question:", sample_row["question"])
for key in ["a", "b", "c", "d"]:
    print(f"{key.upper()}: {sample_row[key]}")

# Cell 16
train_split, val_split = train_test_split(
    train_df,
    test_size=0.1,
    stratify=train_df["answer"],
    random_state=SEED,
)

train_split = train_split.reset_index(drop=True)
val_split = val_split.reset_index(drop=True)

print(f"Training rows: {len(train_split)}")
print(f"Validation rows: {len(val_split)}")
print("Validation label balance:")
display(val_split["answer"].value_counts().sort_index())

# Cell 18
SYSTEM_PROMPT = (
    "You are a Korean visual question answering assistant. "
    "Inspect the provided image carefully and choose the correct option among a, b, c, d. "
    "Respond strictly using a JSON object like {\"answer\": \"a\"}. No explanations, no additional text."
)

CHOICE_KEYS = ["a", "b", "c", "d"]

def create_user_block(row):
    option_lines = "".join(f"{key.upper()}. {row[key]}" for key in CHOICE_KEYS)
    return (f"ì§ˆë¬¸:{row['question']}"f"ì„ íƒì§€:{option_lines}"
        "ì‘ë‹µ í˜•ì‹: JSON ì˜ˆì‹œ {\"answer\": \"a\"} (ì†Œë¬¸ì, ë”°ì˜´í‘œ ìœ ì§€, ì„¤ëª… ê¸ˆì§€)."
    )

def build_messages(row):
    return [
        {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": create_user_block(row)},
            ],
        },
    ]


def load_image(path: str) -> Image.Image:
    with Image.open(path) as img:
        return img.convert("RGB")

print("Prompt helpers ready.")

# Cell 20
import importlib.util
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

if HF_TOKEN:
    login(token=HF_TOKEN, add_to_git_credential=False)
else:
    print("Proceeding without HF login; ensure the model is public for your account.")

processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_ID,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config,
)

model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

candidate_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"]
found_modules = set()
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear) and any(token in name for token in candidate_modules):
        found_modules.add(name.split(".")[-1])
if not found_modules:
    found_modules = {"q_proj", "k_proj", "v_proj", "o_proj"}

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=sorted(found_modules),
)

model = get_peft_model(model, lora_config)
model.config.use_cache = False

if model.generation_config.pad_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.pad_token_id = processor.tokenizer.pad_token_id
if model.generation_config.eos_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.eos_token_id = processor.tokenizer.eos_token_id

if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "right"

print("Model loaded with 4-bit quantization and LoRA adapters. Trainable parameters:")
model.print_trainable_parameters()

# Cell 22
from torch.utils.data import Dataset
import json
import torch

MAX_SEQUENCE_LENGTH = 1024

class VqaFineTuneDataset(Dataset):
    def __init__(self, dataframe):
        self.df = dataframe.reset_index(drop=True)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {
            "image_path": row["image_path"],
            "messages": build_messages(row),
            "answer": str(row["answer"]).lower(),
        }

def _build_conversation(sample):
    # ì •ë‹µ JSONê¹Œì§€ í¬í•¨í•œ ëŒ€í™”(í•™ìŠµ íƒ€ê¹ƒ)
    conversation = sample["messages"] + [
        {
            "role": "assistant",
            "content": [{"type": "text", "text": json.dumps({"answer": sample["answer"]})}],
        }
    ]
    # (1) í”„ë¡¬í”„íŠ¸ë§Œ
    prompt_text = processor.apply_chat_template(
        sample["messages"], tokenize=False, add_generation_prompt=True
    )
    # (2) í”„ë¡¬í”„íŠ¸ + ì •ë‹µJSON
    chat_text = processor.apply_chat_template(
        conversation, tokenize=False, add_generation_prompt=False
    )
    return prompt_text, chat_text, sample["image_path"]

# â˜… ë””ì½”ë” ì „ìš© ëª¨ë¸ì€ left padding í•„ìˆ˜
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

def fine_tune_collate_fn(batch):
    images, prompts, full_texts = [], [], []

    for sample in batch:
        prompt_text, chat_text, image_path = _build_conversation(sample)
        images.append(load_image(image_path))
        prompts.append(prompt_text)
        full_texts.append(chat_text)

    # ğŸ”¹ í”„ë¡¬í”„íŠ¸ë¥¼ "ì´ë¯¸ì§€ í¬í•¨"ìœ¼ë¡œ ì¸ì½”ë”© â†’ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´(ë¹„ì „ í† í° í¬í•¨) íšë“
    enc_prompt = processor(
        images=images,
        text=prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    # ğŸ”¹ í”„ë¡¬í”„íŠ¸+ì •ë‹µ ì „ì²´ ì¸ì½”ë”©
    enc_full = processor(
        images=images,
        text=full_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )

    labels = enc_full["input_ids"].clone()
    # íŒ¨ë”© í† í°ì€ ì†ì‹¤ì—ì„œ ì œì™¸
    labels[enc_full["attention_mask"] == 0] = -100

    # âœ… íŒ¨ë”© ë°©ì‹ê³¼ ë¬´ê´€í•˜ê²Œ "í”„ë¡¬í”„íŠ¸ êµ¬ê°„"ë§Œ ì •í™•íˆ ë§ˆìŠ¤í‚¹
    for i in range(labels.size(0)):
        full_mask   = enc_full["attention_mask"][i].bool()          # ì „ì²´ì—ì„œ non-pad ìœ„ì¹˜
        prompt_len  = int(enc_prompt["attention_mask"][i].sum().item())  # í”„ë¡¬í”„íŠ¸ì˜ non-pad ê¸¸ì´
        nonpad_idx  = torch.nonzero(full_mask, as_tuple=False).squeeze(-1)
        prompt_idx  = nonpad_idx[:prompt_len]   # non-pad ì¤‘ ì•ìª½ prompt_lenê°œê°€ 'í”„ë¡¬í”„íŠ¸ í† í°'
        labels[i, prompt_idx] = -100            # í”„ë¡¬í”„íŠ¸ êµ¬ê°„ ì†ì‹¤ ì œì™¸

    enc_full["labels"] = labels
    return enc_full

train_dataset = VqaFineTuneDataset(train_split)
if len(val_split) <= 128:
    val_eval_df = val_split.copy().reset_index(drop=True)
else:
    val_eval_df = val_split.sample(n=128, random_state=SEED).reset_index(drop=True)
val_dataset = VqaFineTuneDataset(val_eval_df)
print(f"Train dataset size: {len(train_dataset)} | Eval dataset size: {len(val_dataset)}")

# Cell 24
# 12ë²ˆ: TrainingArguments í˜¸í™˜ íŒ¨ì¹˜ + Trainer ì‹¤í–‰ (í‚¤ ë³´ì¡´/ì•ˆì •í™” í¬í•¨)
import inspect
from transformers import TrainingArguments, Trainer

# ë””ì½”ë” ì „ìš© ëª¨ë¸ì€ left-padding ê¶Œì¥
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

OUTPUT_DIR = WORK_DIR / "qwen25vl_lora"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

EPOCHS = 1
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 1e-4

# ê³µí†µ ì¸ì (remove_unused_columns=False ë°˜ë“œì‹œ!)
ta_kwargs = dict(
    output_dir=str(OUTPUT_DIR),
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    logging_steps=20,
    save_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available(),
    optim="paged_adamw_8bit",
    dataloader_num_workers=0,        # â† ì›Œì»¤ 0: ì—ëŸ¬ ë©”ì‹œì§€ ì‚¼í‚´ ë°©ì§€
    max_grad_norm=1.0,
    remove_unused_columns=False,     # â† Trainerê°€ 'image_path/messages'ë¥¼ ì§€ìš°ì§€ ì•Šë„ë¡
)

# ---- í˜¸í™˜ ì²˜ë¦¬: evaluation_strategy / eval_strategy ì¤‘ ìˆëŠ” ê²ƒë§Œ ì‚¬ìš© ----
sig = inspect.signature(TrainingArguments.__init__)
if "evaluation_strategy" in sig.parameters:
    ta_kwargs["evaluation_strategy"] = "epoch"
elif "eval_strategy" in sig.parameters:
    ta_kwargs["eval_strategy"] = "epoch"
# ë‘˜ ë‹¤ ì—†ëŠ” ë²„ì „ì´ë©´ ìë™í‰ê°€ ê±´ë„ˆë›°ê³  ì•„ë˜ì—ì„œ ìˆ˜ë™ evaluate()

training_args = TrainingArguments(**ta_kwargs)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,   # 11ë²ˆì—ì„œ ë§Œë“  VqaFineTuneDataset
    eval_dataset=val_dataset,      # 11ë²ˆì—ì„œ ë§Œë“  VqaFineTuneDataset(ìƒ˜í”Œ 128)
    data_collator=fine_tune_collate_fn,  # 11ë²ˆì˜ collate_fn ì‚¬ìš©
)

train_result = trainer.train()
print("Training metrics:", getattr(train_result, "metrics", {}))

# ìë™ í‰ê°€ ê¸°ëŠ¥ì´ ì—†ëŠ” ë²„ì „ ëŒ€ë¹„: ìˆ˜ë™ í‰ê°€ ì‹œë„
try:
    eval_metrics = trainer.evaluate(eval_dataset=val_dataset)
    print("Eval metrics:", eval_metrics)
except Exception as e:
    print(f"(info) ìë™ í‰ê°€ ì˜µì…˜ ë¯¸ì§€ì› ë˜ëŠ” ê±´ë„ˆëœ€: {e}")

adapter_path = OUTPUT_DIR / "lora_adapter"
model.save_pretrained(adapter_path)                 # LoRA/PEFT ëª¨ë¸ì´ë©´ ì–´ëŒ‘í„° ì €ì¥
processor.save_pretrained(OUTPUT_DIR / "processor")
print(f"LoRA fine-tuning complete. Adapter weights saved to {adapter_path}")

# Cell 26
LETTER_JSON_PATTERN = re.compile(r'"answer"\s*:\s*"([abcd])"')
LETTER_FALLBACK_PATTERN = re.compile(r"([abcd])")

PRED_BATCH_SIZE = 2
MAX_NEW_TOKENS = 24

def decode_answer_text(text: str) -> str:
    lowered = text.lower()
    match = LETTER_JSON_PATTERN.search(lowered)
    if match:
        return match.group(1)
    match = LETTER_FALLBACK_PATTERN.search(lowered)
    if match:
        return match.group(1)
    return "a"


def run_inference(records, batch_size: int = PRED_BATCH_SIZE, keep_raw: bool = False):
    outputs = []
    raw_outputs = [] if keep_raw else None
    for start in tqdm(range(0, len(records), batch_size)):
        batch = records[start : start + batch_size]
        prompts = []
        images = []
        for row in batch:
            prompts.append(
                processor.apply_chat_template(
                    build_messages(row),
                    tokenize=False,
                    add_generation_prompt=True,
                )
            )
            images.append(load_image(row["image_path"]))
        inputs = processor(
            text=prompts,
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        with torch.inference_mode():
            generated = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.0,
                do_sample=False,
            )
        decoded = processor.batch_decode(generated, skip_special_tokens=True)
        for response in decoded:
            answer = decode_answer_text(response)
            outputs.append(answer)
            if keep_raw:
                raw_outputs.append(response)
    if keep_raw:
        return outputs, raw_outputs
    return outputs

print("Inference helpers initialised.")

# Cell 28
# 12. Validation Run (OOM-safe, Kaggle T4 ëŒ€ì‘)
import gc
from tqdm.auto import tqdm
import torch

# Qwen2.5-VL ì€ decoder-only êµ¬ì¡° â†’ ë°˜ë“œì‹œ left padding
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

# ë„ˆë¬´ í¬ê²Œ ëŒë¦¬ë©´ T4(16GB)ì—ì„œ ë°”ë¡œ ì£½ìŒ â†’ ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê³ , ë°°ì¹˜=1ë¡œ ìˆœì°¨ ì²˜ë¦¬
VALIDATION_LIMIT = min(32, len(val_split))  # ê¸°ì¡´ 128 -> 32 (í•„ìš”ì‹œ 16/24ë¡œ ë” ì¤„ì—¬ë„ ë¨)
validation_subset = val_split.sample(VALIDATION_LIMIT, random_state=SEED).reset_index(drop=True)
validation_records = validation_subset.to_dict("records")

val_preds, raw_responses = [], []

def _infer_one(row):
    # ì´ë¯¸ì§€/í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    img_path = row["image_path"] if "image_path" in row else row["image"]  # ë…¸íŠ¸ë¶ ì»¬ëŸ¼ëª…ì— ë§ì¶° ìë™ ëŒ€ì‘
    image = load_image(img_path)
    messages = build_messages(row)
    chat_text = processor.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    # í† í¬ë‚˜ì´ì €+ì´ë¯¸ì§€ ì¸ì½”ë”© (ë°°ì¹˜=1)
    inputs = processor(
        images=[image],
        text=[chat_text],
        return_tensors="pt",
        padding=True,
    ).to(model.device)

    # ì‹¤ì œ ìƒì„±: ë°°ì¹˜=1, no temperature, ì§§ê²Œ, use_cache=False ë¡œ VRAM ì ˆê°
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=8,        # ë„ˆë¬´ ê¸¸ë©´ ë©”ëª¨ë¦¬ íŒ ëŠ˜ì–´ë‚¨
            do_sample=False,         # temperature ì‚¬ìš© ì•ˆ í•¨ (ê²½ê³  ì œê±°)
            use_cache=False          # ìºì‹œ ë„ë©´ ëŠë¦¬ì§€ë§Œ VRAM ì‚¬ìš©ëŸ‰â†“
        )
    text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]

    # ì •ë¦¬
    del inputs, output_ids, image
    gc.collect(); torch.cuda.empty_cache()
    return text

for row in tqdm(validation_records, total=len(validation_records)):
    out_text = _infer_one(row)
    raw_responses.append(out_text)

    # JSON íŒŒì‹± (ë‹¹ì‹  ë…¸íŠ¸ë¶ì˜ íŒŒì„œ í•¨ìˆ˜ëª…ì´ ë‹¤ë¥´ë©´ ë°”ê¿”ì£¼ì„¸ìš”)
    # ì˜ˆ: pred = parse_json_answer(out_text, choice_keys=CHOICE_KEYS, default="a")
    try:
        m = re.search(r'{"\s*answer"\s*:\s*"(a|b|c|d)"}', out_text.strip(), flags=re.I)
        pred = m.group(1).lower() if m else "a"
    except Exception:
        pred = "a"
    val_preds.append(pred)

validation_subset = validation_subset.copy()
validation_subset["prediction"] = val_preds
validation_subset["raw_response"] = raw_responses
validation_subset["is_correct"] = (validation_subset["answer"].str.lower() == validation_subset["prediction"])

val_accuracy = validation_subset["is_correct"].mean()
print(f"âœ… Validation accuracy on {len(validation_subset)} samples: {val_accuracy:.4f}")

display(validation_subset.head())
display(
    validation_subset[validation_subset["is_correct"] == False][
        ["id", "answer", "prediction", "raw_response"]
    ].head()
)

# Cell 30
test_records = test_df.to_dict("records")

test_predictions = run_inference(test_records)
submission = pd.DataFrame({
    "id": test_df["id"],
    "answer": test_predictions,
})

submission_path = WORK_DIR / "submission.csv"
submission.to_csv(submission_path, index=False)
print(f"Saved submission to {submission_path}")

display(submission.head())
