
# Cell 2
import os
import platform
import shutil
import subprocess
from pathlib import Path

import torch

print(f"Python: {platform.python_version()}")
print(f"Torch: {torch.__version__}")
print(f"CUDA devices: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    for idx in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(idx)
        print(f"  GPU {idx}: {props.name} ({props.total_memory / 1024 ** 3:.1f} GB)")
    try:
        subprocess.run(["nvidia-smi"], check=False)
    except FileNotFoundError:
        print("nvidia-smi command not available.")
else:
    print("CUDA not available; training will run on CPU.")

project_root = Path(os.environ.get("PROJECT_ROOT", Path.home() / "workspace"))
project_root.mkdir(parents=True, exist_ok=True)
print(f"Project root: {project_root.resolve()}")

if project_root.exists():
    total, used, free = shutil.disk_usage(project_root)
    print(
        f"Disk usage for {project_root}: total={total/1e9:.1f} GB | "
        f"used={used/1e9:.1f} GB | free={free/1e9:.1f} GB"
    )

# Cell 4
%%capture
%pip install --upgrade transformers accelerate einops tiktoken huggingface_hub qwen-vl-utils peft bitsandbytes python-dotenv

# Cell 6
import json
import math
import os
import random
import re
from collections import Counter
from pathlib import Path

import numpy as np
import pandas as pd
from IPython.display import display
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

import matplotlib.pyplot as plt
import torch
from dotenv import load_dotenv
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoProcessor

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

plt.rcParams["figure.figsize"] = (6, 6)
plt.rcParams["axes.grid"] = False
pd.set_option("display.max_columns", None)

os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.backends.cuda.matmul.allow_tf32 = True
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

PROJECT_ROOT = Path(os.environ.get("PROJECT_ROOT", Path.home() / "workspace"))
PROJECT_ROOT.mkdir(parents=True, exist_ok=True)

WORK_DIR = Path(os.environ.get("WORK_DIR", PROJECT_ROOT / "runs"))
WORK_DIR.mkdir(parents=True, exist_ok=True)

ARTIFACT_DIR = WORK_DIR / "artifacts"
ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)

DATA_KEYWORD = os.environ.get("DATA_KEYWORD", "2025-ssafy-14")
ENV_DATA_ROOT = os.environ.get("DATA_ROOT")
ENV_ARCHIVE_PATH = os.environ.get("DATA_ARCHIVE_PATH")
CUSTOM_SEARCH = os.environ.get("DATA_SEARCH_DIR")

load_dotenv(Path.cwd() / ".env")
HF_TOKEN = os.getenv("HF_TOKEN")

if HF_TOKEN:
    os.environ["HF_TOKEN"] = HF_TOKEN
    print("[OK] HF_TOKEN loaded from .env/environment.")
else:
    print("[WARN] HF_TOKEN not found. Set it in .env for gated Hugging Face models.")

DATA_SEARCH_DIRS = []
if CUSTOM_SEARCH:
    DATA_SEARCH_DIRS.append(Path(CUSTOM_SEARCH).expanduser())
if ENV_DATA_ROOT:
    DATA_SEARCH_DIRS.append(Path(ENV_DATA_ROOT).expanduser())
DATA_SEARCH_DIRS.extend(
    [
        PROJECT_ROOT,
        PROJECT_ROOT / "data",
        Path.home(),
    ]
)

unique_dirs = []
for path in DATA_SEARCH_DIRS:
    if path is None:
        continue
    path = Path(path).expanduser()
    if path not in unique_dirs:
        unique_dirs.append(path)
DATA_SEARCH_DIRS = unique_dirs

# Cell 8
import zipfile

data_keyword = DATA_KEYWORD
DEFAULT_DATA_ROOT = PROJECT_ROOT / data_keyword
DATA_ROOT = None

if ENV_DATA_ROOT:
    candidate = Path(ENV_DATA_ROOT).expanduser()
    if (candidate / "train.csv").exists():
        DATA_ROOT = candidate
        print(f"Using dataset from DATA_ROOT={DATA_ROOT}")

if DATA_ROOT is None:
    for base in DATA_SEARCH_DIRS:
        csv_path = Path(base).expanduser() / data_keyword / "train.csv"
        if csv_path.exists():
            DATA_ROOT = csv_path.parent
            print(f"Detected dataset at {DATA_ROOT}")
            break

if DATA_ROOT is None:
    archive_candidates = []
    if ENV_ARCHIVE_PATH:
        arch = Path(ENV_ARCHIVE_PATH).expanduser()
        if arch.exists():
            archive_candidates.append(arch)
    for base in DATA_SEARCH_DIRS:
        base = Path(base).expanduser()
        candidate = base / f"{data_keyword}.zip"
        if candidate.exists():
            archive_candidates.append(candidate)
    archive_candidates = sorted({p.resolve() for p in archive_candidates})
    if not archive_candidates:
        raise FileNotFoundError(
            "Dataset not found. Set DATA_ROOT or DATA_ARCHIVE_PATH environment variables."
        )
    archive_path = archive_candidates[0]
    DATA_ROOT = DEFAULT_DATA_ROOT
    DATA_ROOT.mkdir(parents=True, exist_ok=True)
    print(f"Extracting {archive_path} -> {DATA_ROOT}")
    with zipfile.ZipFile(archive_path, "r") as zf:
        zf.extractall(DATA_ROOT)
else:
    archive_path = None
    DATA_ROOT.mkdir(parents=True, exist_ok=True)

print(f"DATA_ROOT resolved to: {DATA_ROOT.resolve()}")
print("Contents:")
for child in sorted(DATA_ROOT.iterdir()):
    print(f"- {child.name}")

# Cell 10
train_df = pd.read_csv(DATA_ROOT / "train.csv")
test_df = pd.read_csv(DATA_ROOT / "test.csv")
sample_submission = pd.read_csv(DATA_ROOT / "sample_submission.csv")

train_df["answer"] = train_df["answer"].str.lower()
train_df["image_path"] = train_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))
test_df["image_path"] = test_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))

print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(f"Sample submission shape: {sample_submission.shape}")

display(train_df.head())

# Cell 12
label_distribution = train_df["answer"].value_counts().sort_index()
print("Label distribution (train):")
display(label_distribution.to_frame(name="count"))

duplicate_ids = train_df["id"].duplicated().sum()
print(f"Duplicate train IDs: {duplicate_ids}")

null_summary = train_df.isna().sum()
print("Null counts (train):")
display(null_summary[null_summary > 0])

sample_paths = train_df["image_path"].sample(n=min(256, len(train_df)), random_state=SEED)
widths, heights = [], []
for path in sample_paths:
    with Image.open(path) as img:
        w, h = img.size
    widths.append(w)
    heights.append(h)

print(
    f"Sample width px -> mean: {np.mean(widths):.1f}, min: {min(widths)}, max: {max(widths)}"
)
print(
    f"Sample height px -> mean: {np.mean(heights):.1f}, min: {min(heights)}, max: {max(heights)}"
)

# Cell 14
num_examples = 4
fig, axes = plt.subplots(1, num_examples, figsize=(4 * num_examples, 4))
for ax, (_, row) in zip(axes, train_df.sample(num_examples, random_state=SEED).iterrows()):
    with Image.open(row["image_path"]) as img:
        ax.imshow(img)
    ax.axis("off")
    title_lines = [
        row["id"],
        row["question"][:40] + ("..." if len(row["question"]) > 40 else ""),
        f"Answer: {row['answer'].upper()}"
    ]
    ax.set_title("".join(title_lines), fontsize=9)
plt.tight_layout()

sample_row = train_df.sample(1, random_state=SEED).iloc[0]
print("Sample question:", sample_row["question"])
for key in ["a", "b", "c", "d"]:
    print(f"{key.upper()}: {sample_row[key]}")

# Cell 16
train_split, val_split = train_test_split(
    train_df,
    test_size=0.1,
    stratify=train_df["answer"],
    random_state=SEED,
)

train_split = train_split.reset_index(drop=True)
val_split = val_split.reset_index(drop=True)

print(f"Training rows: {len(train_split)}")
print(f"Validation rows: {len(val_split)}")
print("Validation label balance:")
display(val_split["answer"].value_counts().sort_index())

# Cell 18
SYSTEM_PROMPT = (
    "You are a Korean visual question answering assistant. "
    "Inspect the provided image carefully and choose the correct option among a, b, c, d. "
    "Respond strictly using a JSON object like {\"answer\": \"a\"}. No explanations, no additional text."
)

CHOICE_KEYS = ["a", "b", "c", "d"]

def create_user_block(row):
    option_lines = "".join(f"{key.upper()}. {row[key]}" for key in CHOICE_KEYS)
    return (f"ì§ˆë¬¸:{row['question']}"f"ì„ íƒì§€:{option_lines}"
        "ì‘ë‹µ í˜•ì‹: JSON ì˜ˆì‹œ {\"answer\": \"a\"} (ì†Œë¬¸ì, ë”°ì˜´í‘œ ìœ ì§€, ì„¤ëª… ê¸ˆì§€)."
    )

def build_messages(row):
    return [
        {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": create_user_block(row)},
            ],
        },
    ]


def load_image(path: str) -> Image.Image:
    with Image.open(path) as img:
        return img.convert("RGB")

print("Prompt helpers ready.")

# Cell 20
import importlib.util
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

if HF_TOKEN:
    try:
        login(token=HF_TOKEN, add_to_git_credential=False)
    except Exception as exc:
        print(f"[WARN] Hugging Face login skipped: {exc}")
else:
    print("[WARN] Proceeding without HF login; ensure the model is accessible for your account.")

processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)

compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_ID,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config,
)

model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

target_candidate_modules = [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "up_proj",
    "down_proj",
    "gate_proj",
]
found_modules = set()
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear) and any(token in name for token in target_candidate_modules):
        found_modules.add(name.split(".")[-1])
if not found_modules:
    found_modules = {"q_proj", "k_proj", "v_proj", "o_proj"}

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=sorted(found_modules),
)

model = get_peft_model(model, lora_config)
model.config.use_cache = False

if model.generation_config.pad_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.pad_token_id = processor.tokenizer.pad_token_id
if model.generation_config.eos_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.eos_token_id = processor.tokenizer.eos_token_id

if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "right"

print("Model loaded with 4-bit quantization and LoRA adapters. Trainable parameters:")
model.print_trainable_parameters()

# Cell 22
from torch.utils.data import Dataset
import json
import torch

MAX_SEQUENCE_LENGTH = 1024

class VqaFineTuneDataset(Dataset):
    def __init__(self, dataframe):
        self.df = dataframe.reset_index(drop=True)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {
            "image_path": row["image_path"],
            "messages": build_messages(row),
            "answer": str(row["answer"]).lower(),
        }

def _build_conversation(sample):
    # ì •ë‹µ JSONê¹Œì§€ í¬í•¨í•œ ëŒ€í™”(í•™ìŠµ íƒ€ê¹ƒ)
    conversation = sample["messages"] + [
        {
            "role": "assistant",
            "content": [{"type": "text", "text": json.dumps({"answer": sample["answer"]})}],
        }
    ]
    # (1) í”„ë¡¬í”„íŠ¸ë§Œ
    prompt_text = processor.apply_chat_template(
        sample["messages"], tokenize=False, add_generation_prompt=True
    )
    # (2) í”„ë¡¬í”„íŠ¸ + ì •ë‹µJSON
    chat_text = processor.apply_chat_template(
        conversation, tokenize=False, add_generation_prompt=False
    )
    return prompt_text, chat_text, sample["image_path"]

# â˜… ë””ì½”ë” ì „ìš© ëª¨ë¸ì€ left padding í•„ìˆ˜
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

def fine_tune_collate_fn(batch):
    images, prompts, full_texts = [], [], []

    for sample in batch:
        prompt_text, chat_text, image_path = _build_conversation(sample)
        images.append(load_image(image_path))
        prompts.append(prompt_text)
        full_texts.append(chat_text)

    # ğŸ”¹ í”„ë¡¬í”„íŠ¸ë¥¼ "ì´ë¯¸ì§€ í¬í•¨"ìœ¼ë¡œ ì¸ì½”ë”© â†’ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´(ë¹„ì „ í† í° í¬í•¨) íšë“
    enc_prompt = processor(
        images=images,
        text=prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    # ğŸ”¹ í”„ë¡¬í”„íŠ¸+ì •ë‹µ ì „ì²´ ì¸ì½”ë”©
    enc_full = processor(
        images=images,
        text=full_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )

    labels = enc_full["input_ids"].clone()
    # íŒ¨ë”© í† í°ì€ ì†ì‹¤ì—ì„œ ì œì™¸
    labels[enc_full["attention_mask"] == 0] = -100

    # âœ… íŒ¨ë”© ë°©ì‹ê³¼ ë¬´ê´€í•˜ê²Œ "í”„ë¡¬í”„íŠ¸ êµ¬ê°„"ë§Œ ì •í™•íˆ ë§ˆìŠ¤í‚¹
    for i in range(labels.size(0)):
        full_mask   = enc_full["attention_mask"][i].bool()          # ì „ì²´ì—ì„œ non-pad ìœ„ì¹˜
        prompt_len  = int(enc_prompt["attention_mask"][i].sum().item())  # í”„ë¡¬í”„íŠ¸ì˜ non-pad ê¸¸ì´
        nonpad_idx  = torch.nonzero(full_mask, as_tuple=False).squeeze(-1)
        prompt_idx  = nonpad_idx[:prompt_len]   # non-pad ì¤‘ ì•ìª½ prompt_lenê°œê°€ 'í”„ë¡¬í”„íŠ¸ í† í°'
        labels[i, prompt_idx] = -100            # í”„ë¡¬í”„íŠ¸ êµ¬ê°„ ì†ì‹¤ ì œì™¸

    enc_full["labels"] = labels
    return enc_full

train_dataset = VqaFineTuneDataset(train_split)
if len(val_split) <= 128:
    val_eval_df = val_split.copy().reset_index(drop=True)
else:
    val_eval_df = val_split.sample(n=128, random_state=SEED).reset_index(drop=True)
val_dataset = VqaFineTuneDataset(val_eval_df)
print(f"Train dataset size: {len(train_dataset)} | Eval dataset size: {len(val_dataset)}")

# Cell 24
# 12ë‹¨ê³„: TrainingArguments í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • + Trainer ì‹¤í–‰ (ì„±ëŠ¥/ì•ˆì •ì„± ìš°ì„ )
import inspect
from transformers import TrainingArguments, Trainer

if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

OUTPUT_DIR = WORK_DIR / "qwen25vl_lora"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

GPU_MEM_GB = 0.0
if torch.cuda.is_available():
    props = torch.cuda.get_device_properties(0)
    GPU_MEM_GB = props.total_memory / 1024 ** 3

PER_DEVICE_BATCH = 2 if GPU_MEM_GB >= 22 else 1
TARGET_EFFECTIVE_BATCH = int(os.environ.get("EFFECTIVE_BATCH_SIZE", 16))
GRADIENT_ACCUMULATION = max(1, TARGET_EFFECTIVE_BATCH // PER_DEVICE_BATCH)
EPOCHS = int(os.environ.get("NUM_EPOCHS", 3))
LEARNING_RATE = float(os.environ.get("LEARNING_RATE", 5e-5))
WARMUP_RATIO = float(os.environ.get("WARMUP_RATIO", 0.05))

print(
    f"GPU memory: {GPU_MEM_GB:.1f} GB | per_device_batch={PER_DEVICE_BATCH} | "
    f"grad_accum={GRADIENT_ACCUMULATION} | epochs={EPOCHS}"
)

bf16_enabled = torch.cuda.is_available() and torch.cuda.is_bf16_supported()

common_kwargs = dict(
    output_dir=str(OUTPUT_DIR),
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=PER_DEVICE_BATCH,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    warmup_ratio=WARMUP_RATIO,
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=3,
    report_to="none",
    bf16=bf16_enabled,
    fp16=not bf16_enabled,
    optim="adamw_bnb_8bit",
    dataloader_num_workers=2,
    gradient_checkpointing=True,
    max_grad_norm=0.3,
    remove_unused_columns=False,
)

max_steps_env = os.environ.get("MAX_TRAIN_STEPS")
if max_steps_env:
    common_kwargs["max_steps"] = int(max_steps_env)

sig = inspect.signature(TrainingArguments.__init__)
if "evaluation_strategy" in sig.parameters:
    common_kwargs["evaluation_strategy"] = "epoch"
elif "eval_strategy" in sig.parameters:
    common_kwargs["eval_strategy"] = "epoch"

training_args = TrainingArguments(**common_kwargs)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=fine_tune_collate_fn,
)

train_result = trainer.train()
print("Training metrics:", getattr(train_result, "metrics", {}))

try:
    eval_metrics = trainer.evaluate(eval_dataset=val_dataset)
    print("Eval metrics:", eval_metrics)
except Exception as e:
    print(f"(info) Evaluation skipped due to: {e}")

adapter_path = OUTPUT_DIR / "lora_adapter"
model.save_pretrained(adapter_path)
processor.save_pretrained(OUTPUT_DIR / "processor")
print(f"LoRA fine-tuning complete. Adapter weights saved to {adapter_path}")

# Cell 26
LETTER_JSON_PATTERN = re.compile(r'"answer"\s*:\s*"([abcd])"')
LETTER_FALLBACK_PATTERN = re.compile(r"([abcd])")

PRED_BATCH_SIZE = 2
MAX_NEW_TOKENS = 24

def decode_answer_text(text: str) -> str:
    lowered = text.lower()
    match = LETTER_JSON_PATTERN.search(lowered)
    if match:
        return match.group(1)
    match = LETTER_FALLBACK_PATTERN.search(lowered)
    if match:
        return match.group(1)
    return "a"


def run_inference(records, batch_size: int = PRED_BATCH_SIZE, keep_raw: bool = False):
    outputs = []
    raw_outputs = [] if keep_raw else None
    for start in tqdm(range(0, len(records), batch_size)):
        batch = records[start : start + batch_size]
        prompts = []
        images = []
        for row in batch:
            prompts.append(
                processor.apply_chat_template(
                    build_messages(row),
                    tokenize=False,
                    add_generation_prompt=True,
                )
            )
            images.append(load_image(row["image_path"]))
        inputs = processor(
            text=prompts,
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        with torch.inference_mode():
            generated = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.0,
                do_sample=False,
            )
        decoded = processor.batch_decode(generated, skip_special_tokens=True)
        for response in decoded:
            answer = decode_answer_text(response)
            outputs.append(answer)
            if keep_raw:
                raw_outputs.append(response)
    if keep_raw:
        return outputs, raw_outputs
    return outputs

print("Inference helpers initialised.")

# Cell 28
# 12. Validation Run (OOM-safe, Kaggle T4 ëŒ€ì‘)
import gc
from tqdm.auto import tqdm
import torch

# Qwen2.5-VL ì€ decoder-only êµ¬ì¡° â†’ ë°˜ë“œì‹œ left padding
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

# ë„ˆë¬´ í¬ê²Œ ëŒë¦¬ë©´ T4(16GB)ì—ì„œ ë°”ë¡œ ì£½ìŒ â†’ ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê³ , ë°°ì¹˜=1ë¡œ ìˆœì°¨ ì²˜ë¦¬
VALIDATION_LIMIT = min(32, len(val_split))  # ê¸°ì¡´ 128 -> 32 (í•„ìš”ì‹œ 16/24ë¡œ ë” ì¤„ì—¬ë„ ë¨)
validation_subset = val_split.sample(VALIDATION_LIMIT, random_state=SEED).reset_index(drop=True)
validation_records = validation_subset.to_dict("records")

val_preds, raw_responses = [], []

def _infer_one(row):
    # ì´ë¯¸ì§€/í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    img_path = row["image_path"] if "image_path" in row else row["image"]  # ë…¸íŠ¸ë¶ ì»¬ëŸ¼ëª…ì— ë§ì¶° ìë™ ëŒ€ì‘
    image = load_image(img_path)
    messages = build_messages(row)
    chat_text = processor.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    # í† í¬ë‚˜ì´ì €+ì´ë¯¸ì§€ ì¸ì½”ë”© (ë°°ì¹˜=1)
    inputs = processor(
        images=[image],
        text=[chat_text],
        return_tensors="pt",
        padding=True,
    ).to(model.device)

    # ì‹¤ì œ ìƒì„±: ë°°ì¹˜=1, no temperature, ì§§ê²Œ, use_cache=False ë¡œ VRAM ì ˆê°
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=8,        # ë„ˆë¬´ ê¸¸ë©´ ë©”ëª¨ë¦¬ íŒ ëŠ˜ì–´ë‚¨
            do_sample=False,         # temperature ì‚¬ìš© ì•ˆ í•¨ (ê²½ê³  ì œê±°)
            use_cache=False          # ìºì‹œ ë„ë©´ ëŠë¦¬ì§€ë§Œ VRAM ì‚¬ìš©ëŸ‰â†“
        )
    text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]

    # ì •ë¦¬
    del inputs, output_ids, image
    gc.collect(); torch.cuda.empty_cache()
    return text

for row in tqdm(validation_records, total=len(validation_records)):
    out_text = _infer_one(row)
    raw_responses.append(out_text)

    # JSON íŒŒì‹± (ë‹¹ì‹  ë…¸íŠ¸ë¶ì˜ íŒŒì„œ í•¨ìˆ˜ëª…ì´ ë‹¤ë¥´ë©´ ë°”ê¿”ì£¼ì„¸ìš”)
    # ì˜ˆ: pred = parse_json_answer(out_text, choice_keys=CHOICE_KEYS, default="a")
    try:
        m = re.search(r'{"\s*answer"\s*:\s*"(a|b|c|d)"}', out_text.strip(), flags=re.I)
        pred = m.group(1).lower() if m else "a"
    except Exception:
        pred = "a"
    val_preds.append(pred)

validation_subset = validation_subset.copy()
validation_subset["prediction"] = val_preds
validation_subset["raw_response"] = raw_responses
validation_subset["is_correct"] = (validation_subset["answer"].str.lower() == validation_subset["prediction"])

val_accuracy = validation_subset["is_correct"].mean()
print(f"âœ… Validation accuracy on {len(validation_subset)} samples: {val_accuracy:.4f}")

display(validation_subset.head())
display(
    validation_subset[validation_subset["is_correct"] == False][
        ["id", "answer", "prediction", "raw_response"]
    ].head()
)

# Cell 30
test_records = test_df.to_dict("records")

test_predictions = run_inference(test_records)
submission = pd.DataFrame({
    "id": test_df["id"],
    "answer": test_predictions,
})

submission_path = WORK_DIR / "submission.csv"
submission.to_csv(submission_path, index=False)
print(f"Saved submission to {submission_path}")

display(submission.head())
