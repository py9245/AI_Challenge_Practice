{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1460,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 0.32458963990211487,
      "learning_rate": 7.258064516129033e-07,
      "loss": 0.0345,
      "step": 10
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 0.3352462947368622,
      "learning_rate": 1.5322580645161292e-06,
      "loss": 0.0164,
      "step": 20
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 0.36451825499534607,
      "learning_rate": 2.338709677419355e-06,
      "loss": 0.038,
      "step": 30
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 0.30779263377189636,
      "learning_rate": 3.145161290322581e-06,
      "loss": 0.0278,
      "step": 40
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 0.12766233086585999,
      "learning_rate": 3.951612903225807e-06,
      "loss": 0.0287,
      "step": 50
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 0.08368513733148575,
      "learning_rate": 4.758064516129033e-06,
      "loss": 0.0217,
      "step": 60
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 0.3258029818534851,
      "learning_rate": 4.999846115781729e-06,
      "loss": 0.0174,
      "step": 70
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 0.24710652232170105,
      "learning_rate": 4.99909244276641e-06,
      "loss": 0.0214,
      "step": 80
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 0.1636500507593155,
      "learning_rate": 4.997710905619523e-06,
      "loss": 0.0165,
      "step": 90
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 0.3367480933666229,
      "learning_rate": 4.995701851434118e-06,
      "loss": 0.0302,
      "step": 100
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 0.271185040473938,
      "learning_rate": 4.993065784958655e-06,
      "loss": 0.0456,
      "step": 110
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 0.33894261717796326,
      "learning_rate": 4.98980336847019e-06,
      "loss": 0.0284,
      "step": 120
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 0.49479591846466064,
      "learning_rate": 4.985915421607991e-06,
      "loss": 0.0273,
      "step": 130
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 0.09848782420158386,
      "learning_rate": 4.981402921167608e-06,
      "loss": 0.031,
      "step": 140
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 0.17552176117897034,
      "learning_rate": 4.976267000855469e-06,
      "loss": 0.027,
      "step": 150
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 0.2756041884422302,
      "learning_rate": 4.970508951004056e-06,
      "loss": 0.0304,
      "step": 160
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 0.05108048394322395,
      "learning_rate": 4.964130218247715e-06,
      "loss": 0.0296,
      "step": 170
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 0.46411895751953125,
      "learning_rate": 4.957132405159222e-06,
      "loss": 0.0392,
      "step": 180
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 0.19455543160438538,
      "learning_rate": 4.949517269847141e-06,
      "loss": 0.0146,
      "step": 190
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 0.08436702936887741,
      "learning_rate": 4.9412867255141385e-06,
      "loss": 0.0189,
      "step": 200
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 0.2268954962491989,
      "learning_rate": 4.932442839976307e-06,
      "loss": 0.0139,
      "step": 210
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 0.4096520245075226,
      "learning_rate": 4.922987835143655e-06,
      "loss": 0.0251,
      "step": 220
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 0.19776517152786255,
      "learning_rate": 4.912924086461884e-06,
      "loss": 0.021,
      "step": 230
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 0.34356358647346497,
      "learning_rate": 4.9022541223155866e-06,
      "loss": 0.0262,
      "step": 240
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 0.66025310754776,
      "learning_rate": 4.890980623393021e-06,
      "loss": 0.0301,
      "step": 250
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 0.04222177341580391,
      "learning_rate": 4.879106422012629e-06,
      "loss": 0.0241,
      "step": 260
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 0.33756157755851746,
      "learning_rate": 4.8666345014114495e-06,
      "loss": 0.0436,
      "step": 270
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 0.5391907691955566,
      "learning_rate": 4.853567994995618e-06,
      "loss": 0.0321,
      "step": 280
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 0.4055587351322174,
      "learning_rate": 4.839910185553145e-06,
      "loss": 0.0256,
      "step": 290
    },
    {
      "epoch": 1.0273972602739727,
      "grad_norm": 0.2560183107852936,
      "learning_rate": 4.825664504429153e-06,
      "loss": 0.0332,
      "step": 300
    },
    {
      "epoch": 1.0616438356164384,
      "grad_norm": 0.28145402669906616,
      "learning_rate": 4.810834530663797e-06,
      "loss": 0.023,
      "step": 310
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 0.17729079723358154,
      "learning_rate": 4.795423990093082e-06,
      "loss": 0.0239,
      "step": 320
    },
    {
      "epoch": 1.13013698630137,
      "grad_norm": 0.2611127495765686,
      "learning_rate": 4.779436754412791e-06,
      "loss": 0.013,
      "step": 330
    },
    {
      "epoch": 1.1643835616438356,
      "grad_norm": 0.20872920751571655,
      "learning_rate": 4.762876840205773e-06,
      "loss": 0.0119,
      "step": 340
    },
    {
      "epoch": 1.1986301369863013,
      "grad_norm": 0.19450898468494415,
      "learning_rate": 4.745748407932832e-06,
      "loss": 0.0234,
      "step": 350
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 0.2972724735736847,
      "learning_rate": 4.7280557608874645e-06,
      "loss": 0.025,
      "step": 360
    },
    {
      "epoch": 1.2671232876712328,
      "grad_norm": 0.10254504531621933,
      "learning_rate": 4.7098033441147125e-06,
      "loss": 0.0183,
      "step": 370
    },
    {
      "epoch": 1.3013698630136985,
      "grad_norm": 0.12499736994504929,
      "learning_rate": 4.690995743294404e-06,
      "loss": 0.0181,
      "step": 380
    },
    {
      "epoch": 1.3356164383561644,
      "grad_norm": 0.6015827655792236,
      "learning_rate": 4.6716376835890656e-06,
      "loss": 0.021,
      "step": 390
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 0.2919952869415283,
      "learning_rate": 4.6517340284567795e-06,
      "loss": 0.0185,
      "step": 400
    },
    {
      "epoch": 1.404109589041096,
      "grad_norm": 0.1826498955488205,
      "learning_rate": 4.6312897784293136e-06,
      "loss": 0.0278,
      "step": 410
    },
    {
      "epoch": 1.4383561643835616,
      "grad_norm": 0.3195760250091553,
      "learning_rate": 4.6103100698557935e-06,
      "loss": 0.0282,
      "step": 420
    },
    {
      "epoch": 1.4726027397260273,
      "grad_norm": 0.0036555675324052572,
      "learning_rate": 4.5888001736122715e-06,
      "loss": 0.0132,
      "step": 430
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 0.3320177495479584,
      "learning_rate": 4.5667654937774815e-06,
      "loss": 0.0222,
      "step": 440
    },
    {
      "epoch": 1.541095890410959,
      "grad_norm": 0.10509506613016129,
      "learning_rate": 4.544211566275138e-06,
      "loss": 0.0115,
      "step": 450
    },
    {
      "epoch": 1.5753424657534247,
      "grad_norm": 0.6430231332778931,
      "learning_rate": 4.521144057483107e-06,
      "loss": 0.0313,
      "step": 460
    },
    {
      "epoch": 1.6095890410958904,
      "grad_norm": 0.28408828377723694,
      "learning_rate": 4.497568762809805e-06,
      "loss": 0.02,
      "step": 470
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 0.4058386981487274,
      "learning_rate": 4.473491605238178e-06,
      "loss": 0.0192,
      "step": 480
    },
    {
      "epoch": 1.678082191780822,
      "grad_norm": 0.027567604556679726,
      "learning_rate": 4.448918633837626e-06,
      "loss": 0.0188,
      "step": 490
    },
    {
      "epoch": 1.7123287671232876,
      "grad_norm": 0.1403457373380661,
      "learning_rate": 4.4238560222442615e-06,
      "loss": 0.0257,
      "step": 500
    },
    {
      "epoch": 1.7465753424657535,
      "grad_norm": 0.24522821605205536,
      "learning_rate": 4.3983100671098586e-06,
      "loss": 0.0228,
      "step": 510
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 0.15995745360851288,
      "learning_rate": 4.372287186519903e-06,
      "loss": 0.0097,
      "step": 520
    },
    {
      "epoch": 1.8150684931506849,
      "grad_norm": 0.3117015063762665,
      "learning_rate": 4.345793918381134e-06,
      "loss": 0.0199,
      "step": 530
    },
    {
      "epoch": 1.8493150684931505,
      "grad_norm": 0.21654903888702393,
      "learning_rate": 4.3188369187789784e-06,
      "loss": 0.0136,
      "step": 540
    },
    {
      "epoch": 1.8835616438356164,
      "grad_norm": 0.209788516163826,
      "learning_rate": 4.2914229603053016e-06,
      "loss": 0.0167,
      "step": 550
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 0.32458916306495667,
      "learning_rate": 4.263558930356877e-06,
      "loss": 0.015,
      "step": 560
    },
    {
      "epoch": 1.952054794520548,
      "grad_norm": 0.012419109232723713,
      "learning_rate": 4.235251829405024e-06,
      "loss": 0.0164,
      "step": 570
    },
    {
      "epoch": 1.9863013698630136,
      "grad_norm": 0.3361894488334656,
      "learning_rate": 4.2065087692368256e-06,
      "loss": 0.0212,
      "step": 580
    },
    {
      "epoch": 2.0205479452054793,
      "grad_norm": 0.09797172248363495,
      "learning_rate": 4.177336971168389e-06,
      "loss": 0.0221,
      "step": 590
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 0.12627959251403809,
      "learning_rate": 4.147743764230585e-06,
      "loss": 0.0103,
      "step": 600
    },
    {
      "epoch": 2.089041095890411,
      "grad_norm": 0.03036600723862648,
      "learning_rate": 4.117736583327724e-06,
      "loss": 0.0144,
      "step": 610
    },
    {
      "epoch": 2.1232876712328768,
      "grad_norm": 0.16756586730480194,
      "learning_rate": 4.087322967369632e-06,
      "loss": 0.0107,
      "step": 620
    },
    {
      "epoch": 2.1575342465753424,
      "grad_norm": 0.04343920946121216,
      "learning_rate": 4.056510557377607e-06,
      "loss": 0.0083,
      "step": 630
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 0.14098525047302246,
      "learning_rate": 4.025307094564701e-06,
      "loss": 0.0123,
      "step": 640
    },
    {
      "epoch": 2.2260273972602738,
      "grad_norm": 1.3622153997421265,
      "learning_rate": 3.993720418390849e-06,
      "loss": 0.0181,
      "step": 650
    },
    {
      "epoch": 2.26027397260274,
      "grad_norm": 0.17122530937194824,
      "learning_rate": 3.9617584645933085e-06,
      "loss": 0.0061,
      "step": 660
    },
    {
      "epoch": 2.2945205479452055,
      "grad_norm": 0.41828757524490356,
      "learning_rate": 3.929429263192906e-06,
      "loss": 0.0176,
      "step": 670
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 0.6638104319572449,
      "learning_rate": 3.896740936476603e-06,
      "loss": 0.0181,
      "step": 680
    },
    {
      "epoch": 2.363013698630137,
      "grad_norm": 0.0020079289097338915,
      "learning_rate": 3.8637016969568755e-06,
      "loss": 0.0123,
      "step": 690
    },
    {
      "epoch": 2.3972602739726026,
      "grad_norm": 0.7076249718666077,
      "learning_rate": 3.83031984530843e-06,
      "loss": 0.0384,
      "step": 700
    },
    {
      "epoch": 2.4315068493150687,
      "grad_norm": 0.34688282012939453,
      "learning_rate": 3.7966037682827683e-06,
      "loss": 0.0315,
      "step": 710
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 0.33905357122421265,
      "learning_rate": 3.7625619366011234e-06,
      "loss": 0.0103,
      "step": 720
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.38411280512809753,
      "learning_rate": 3.7282029028263064e-06,
      "loss": 0.0172,
      "step": 730
    },
    {
      "epoch": 2.5342465753424657,
      "grad_norm": 0.18945571780204773,
      "learning_rate": 3.6935352992139844e-06,
      "loss": 0.0031,
      "step": 740
    },
    {
      "epoch": 2.5684931506849313,
      "grad_norm": 0.5222576260566711,
      "learning_rate": 3.658567835543942e-06,
      "loss": 0.0204,
      "step": 750
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 0.07659266144037247,
      "learning_rate": 3.6233092969318607e-06,
      "loss": 0.0184,
      "step": 760
    },
    {
      "epoch": 2.636986301369863,
      "grad_norm": 0.20666532218456268,
      "learning_rate": 3.5877685416221757e-06,
      "loss": 0.0138,
      "step": 770
    },
    {
      "epoch": 2.671232876712329,
      "grad_norm": 0.3397608995437622,
      "learning_rate": 3.551954498762561e-06,
      "loss": 0.0105,
      "step": 780
    },
    {
      "epoch": 2.7054794520547945,
      "grad_norm": 0.1760360449552536,
      "learning_rate": 3.5158761661605934e-06,
      "loss": 0.0157,
      "step": 790
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 0.05608443170785904,
      "learning_rate": 3.479542608023173e-06,
      "loss": 0.0047,
      "step": 800
    },
    {
      "epoch": 2.7739726027397262,
      "grad_norm": 0.5246906280517578,
      "learning_rate": 3.442962952679265e-06,
      "loss": 0.0161,
      "step": 810
    },
    {
      "epoch": 2.808219178082192,
      "grad_norm": 0.11456641554832458,
      "learning_rate": 3.40614639028652e-06,
      "loss": 0.016,
      "step": 820
    },
    {
      "epoch": 2.8424657534246576,
      "grad_norm": 0.374604731798172,
      "learning_rate": 3.3691021705223725e-06,
      "loss": 0.0134,
      "step": 830
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 0.011252794414758682,
      "learning_rate": 3.3318396002601794e-06,
      "loss": 0.0043,
      "step": 840
    },
    {
      "epoch": 2.910958904109589,
      "grad_norm": 0.034399084746837616,
      "learning_rate": 3.2943680412309907e-06,
      "loss": 0.0063,
      "step": 850
    },
    {
      "epoch": 2.9452054794520546,
      "grad_norm": 0.3940301537513733,
      "learning_rate": 3.256696907671538e-06,
      "loss": 0.0186,
      "step": 860
    },
    {
      "epoch": 2.9794520547945207,
      "grad_norm": 0.009146841242909431,
      "learning_rate": 3.218835663959028e-06,
      "loss": 0.018,
      "step": 870
    },
    {
      "epoch": 3.0136986301369864,
      "grad_norm": 0.08816040307283401,
      "learning_rate": 3.1807938222333478e-06,
      "loss": 0.0187,
      "step": 880
    },
    {
      "epoch": 3.047945205479452,
      "grad_norm": 0.0218262430280447,
      "learning_rate": 3.1425809400072604e-06,
      "loss": 0.0072,
      "step": 890
    },
    {
      "epoch": 3.0821917808219177,
      "grad_norm": 0.04582212492823601,
      "learning_rate": 3.1042066177652075e-06,
      "loss": 0.0048,
      "step": 900
    },
    {
      "epoch": 3.1164383561643834,
      "grad_norm": 0.25627511739730835,
      "learning_rate": 3.0656804965513123e-06,
      "loss": 0.0113,
      "step": 910
    },
    {
      "epoch": 3.1506849315068495,
      "grad_norm": 0.5633547902107239,
      "learning_rate": 3.0270122555471944e-06,
      "loss": 0.0145,
      "step": 920
    },
    {
      "epoch": 3.184931506849315,
      "grad_norm": 0.009726285934448242,
      "learning_rate": 2.9882116096402024e-06,
      "loss": 0.0091,
      "step": 930
    },
    {
      "epoch": 3.219178082191781,
      "grad_norm": 0.22324806451797485,
      "learning_rate": 2.9492883069826794e-06,
      "loss": 0.0094,
      "step": 940
    },
    {
      "epoch": 3.2534246575342465,
      "grad_norm": 0.04753141477704048,
      "learning_rate": 2.9102521265428622e-06,
      "loss": 0.0114,
      "step": 950
    },
    {
      "epoch": 3.287671232876712,
      "grad_norm": 0.45782434940338135,
      "learning_rate": 2.8711128756480473e-06,
      "loss": 0.0072,
      "step": 960
    },
    {
      "epoch": 3.3219178082191783,
      "grad_norm": 0.15268868207931519,
      "learning_rate": 2.83188038752063e-06,
      "loss": 0.0101,
      "step": 970
    },
    {
      "epoch": 3.356164383561644,
      "grad_norm": 0.4802421033382416,
      "learning_rate": 2.7925645188076324e-06,
      "loss": 0.0113,
      "step": 980
    },
    {
      "epoch": 3.3904109589041096,
      "grad_norm": 0.024233287200331688,
      "learning_rate": 2.753175147104355e-06,
      "loss": 0.0148,
      "step": 990
    },
    {
      "epoch": 3.4246575342465753,
      "grad_norm": 0.8048267364501953,
      "learning_rate": 2.7137221684727578e-06,
      "loss": 0.0092,
      "step": 1000
    },
    {
      "epoch": 3.458904109589041,
      "grad_norm": 0.5819321274757385,
      "learning_rate": 2.6742154949552057e-06,
      "loss": 0.0078,
      "step": 1010
    },
    {
      "epoch": 3.493150684931507,
      "grad_norm": 0.19205336272716522,
      "learning_rate": 2.6346650520841983e-06,
      "loss": 0.0036,
      "step": 1020
    },
    {
      "epoch": 3.5273972602739727,
      "grad_norm": 0.4187041223049164,
      "learning_rate": 2.5950807763887086e-06,
      "loss": 0.0037,
      "step": 1030
    },
    {
      "epoch": 3.5616438356164384,
      "grad_norm": 0.20366479456424713,
      "learning_rate": 2.555472612897765e-06,
      "loss": 0.0236,
      "step": 1040
    },
    {
      "epoch": 3.595890410958904,
      "grad_norm": 0.6768131852149963,
      "learning_rate": 2.5158505126418858e-06,
      "loss": 0.0165,
      "step": 1050
    },
    {
      "epoch": 3.6301369863013697,
      "grad_norm": 0.10947217792272568,
      "learning_rate": 2.4762244301530246e-06,
      "loss": 0.0048,
      "step": 1060
    },
    {
      "epoch": 3.6643835616438354,
      "grad_norm": 0.002865307731553912,
      "learning_rate": 2.4366043209636115e-06,
      "loss": 0.0059,
      "step": 1070
    },
    {
      "epoch": 3.6986301369863015,
      "grad_norm": 0.2418820708990097,
      "learning_rate": 2.3970001391053716e-06,
      "loss": 0.0092,
      "step": 1080
    },
    {
      "epoch": 3.732876712328767,
      "grad_norm": 0.013303892686963081,
      "learning_rate": 2.35742183460849e-06,
      "loss": 0.0098,
      "step": 1090
    },
    {
      "epoch": 3.767123287671233,
      "grad_norm": 0.9267889857292175,
      "learning_rate": 2.31787935100181e-06,
      "loss": 0.0078,
      "step": 1100
    },
    {
      "epoch": 3.8013698630136985,
      "grad_norm": 1.2294062376022339,
      "learning_rate": 2.278382622814644e-06,
      "loss": 0.0129,
      "step": 1110
    },
    {
      "epoch": 3.8356164383561646,
      "grad_norm": 0.1872231364250183,
      "learning_rate": 2.238941573080859e-06,
      "loss": 0.0123,
      "step": 1120
    },
    {
      "epoch": 3.8698630136986303,
      "grad_norm": 0.038569193333387375,
      "learning_rate": 2.1995661108458397e-06,
      "loss": 0.0186,
      "step": 1130
    },
    {
      "epoch": 3.904109589041096,
      "grad_norm": 0.14251498878002167,
      "learning_rate": 2.1602661286769744e-06,
      "loss": 0.0143,
      "step": 1140
    },
    {
      "epoch": 3.9383561643835616,
      "grad_norm": 0.04061037302017212,
      "learning_rate": 2.12105150017828e-06,
      "loss": 0.0049,
      "step": 1150
    },
    {
      "epoch": 3.9726027397260273,
      "grad_norm": 0.018007997423410416,
      "learning_rate": 2.0819320775097806e-06,
      "loss": 0.006,
      "step": 1160
    },
    {
      "epoch": 4.006849315068493,
      "grad_norm": 0.656933069229126,
      "learning_rate": 2.0429176889122902e-06,
      "loss": 0.0132,
      "step": 1170
    },
    {
      "epoch": 4.041095890410959,
      "grad_norm": 1.1655889749526978,
      "learning_rate": 2.0040181362381924e-06,
      "loss": 0.0051,
      "step": 1180
    },
    {
      "epoch": 4.075342465753424,
      "grad_norm": 1.2941423654556274,
      "learning_rate": 1.9652431924888518e-06,
      "loss": 0.0123,
      "step": 1190
    },
    {
      "epoch": 4.109589041095891,
      "grad_norm": 0.6795392632484436,
      "learning_rate": 1.9266025993592827e-06,
      "loss": 0.0107,
      "step": 1200
    },
    {
      "epoch": 4.1438356164383565,
      "grad_norm": 0.8219927549362183,
      "learning_rate": 1.8881060647906662e-06,
      "loss": 0.0062,
      "step": 1210
    },
    {
      "epoch": 4.178082191780822,
      "grad_norm": 0.0377044714987278,
      "learning_rate": 1.8497632605313704e-06,
      "loss": 0.0033,
      "step": 1220
    },
    {
      "epoch": 4.212328767123288,
      "grad_norm": 0.04495127499103546,
      "learning_rate": 1.8115838197070373e-06,
      "loss": 0.0028,
      "step": 1230
    },
    {
      "epoch": 4.2465753424657535,
      "grad_norm": 0.18817098438739777,
      "learning_rate": 1.7735773344003948e-06,
      "loss": 0.0024,
      "step": 1240
    },
    {
      "epoch": 4.280821917808219,
      "grad_norm": 0.030711239203810692,
      "learning_rate": 1.7357533532413725e-06,
      "loss": 0.0014,
      "step": 1250
    },
    {
      "epoch": 4.315068493150685,
      "grad_norm": 2.915780782699585,
      "learning_rate": 1.698121379008128e-06,
      "loss": 0.0169,
      "step": 1260
    },
    {
      "epoch": 4.3493150684931505,
      "grad_norm": 0.333717405796051,
      "learning_rate": 1.6606908662396054e-06,
      "loss": 0.0084,
      "step": 1270
    },
    {
      "epoch": 4.383561643835616,
      "grad_norm": 1.3840546607971191,
      "learning_rate": 1.6234712188601999e-06,
      "loss": 0.0174,
      "step": 1280
    },
    {
      "epoch": 4.417808219178082,
      "grad_norm": 0.04032805562019348,
      "learning_rate": 1.5864717878171532e-06,
      "loss": 0.0084,
      "step": 1290
    },
    {
      "epoch": 4.4520547945205475,
      "grad_norm": 0.002311848569661379,
      "learning_rate": 1.5497018687312435e-06,
      "loss": 0.0079,
      "step": 1300
    },
    {
      "epoch": 4.486301369863014,
      "grad_norm": 0.0036775197368115187,
      "learning_rate": 1.5131706995613868e-06,
      "loss": 0.0107,
      "step": 1310
    },
    {
      "epoch": 4.52054794520548,
      "grad_norm": 0.09878107905387878,
      "learning_rate": 1.4768874582837222e-06,
      "loss": 0.0177,
      "step": 1320
    },
    {
      "epoch": 4.554794520547945,
      "grad_norm": 0.4183354377746582,
      "learning_rate": 1.440861260585762e-06,
      "loss": 0.0106,
      "step": 1330
    },
    {
      "epoch": 4.589041095890411,
      "grad_norm": 0.08925862610340118,
      "learning_rate": 1.4051011575762e-06,
      "loss": 0.0059,
      "step": 1340
    },
    {
      "epoch": 4.623287671232877,
      "grad_norm": 0.005489943083375692,
      "learning_rate": 1.3696161335109328e-06,
      "loss": 0.0085,
      "step": 1350
    },
    {
      "epoch": 4.657534246575342,
      "grad_norm": 0.01947643980383873,
      "learning_rate": 1.3344151035358912e-06,
      "loss": 0.0023,
      "step": 1360
    },
    {
      "epoch": 4.691780821917808,
      "grad_norm": 0.04937811940908432,
      "learning_rate": 1.299506911447223e-06,
      "loss": 0.0067,
      "step": 1370
    },
    {
      "epoch": 4.726027397260274,
      "grad_norm": 0.02112816646695137,
      "learning_rate": 1.2649003274694021e-06,
      "loss": 0.0023,
      "step": 1380
    },
    {
      "epoch": 4.760273972602739,
      "grad_norm": 0.1673058420419693,
      "learning_rate": 1.2306040460518337e-06,
      "loss": 0.0064,
      "step": 1390
    },
    {
      "epoch": 4.794520547945205,
      "grad_norm": 0.0017295217840000987,
      "learning_rate": 1.1966266836844776e-06,
      "loss": 0.0115,
      "step": 1400
    },
    {
      "epoch": 4.828767123287671,
      "grad_norm": 0.2571772038936615,
      "learning_rate": 1.1629767767330777e-06,
      "loss": 0.0048,
      "step": 1410
    },
    {
      "epoch": 4.863013698630137,
      "grad_norm": 0.0006938096485100687,
      "learning_rate": 1.1296627792945078e-06,
      "loss": 0.0016,
      "step": 1420
    },
    {
      "epoch": 4.897260273972603,
      "grad_norm": 0.09463120251893997,
      "learning_rate": 1.0966930610727882e-06,
      "loss": 0.0063,
      "step": 1430
    },
    {
      "epoch": 4.931506849315069,
      "grad_norm": 0.003585649188607931,
      "learning_rate": 1.0640759052763153e-06,
      "loss": 0.0056,
      "step": 1440
    },
    {
      "epoch": 4.965753424657534,
      "grad_norm": 0.00518536102026701,
      "learning_rate": 1.031819506536805e-06,
      "loss": 0.0015,
      "step": 1450
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.3286346197128296,
      "learning_rate": 9.999319688505073e-07,
      "loss": 0.0039,
      "step": 1460
    }
  ],
  "logging_steps": 10,
  "max_steps": 2044,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.131534444576276e+17,
  "train_batch_size": 12,
  "trial_name": null,
  "trial_params": null
}
