{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb06f314",
   "metadata": {},
   "source": [
    "# QLoRA 파인튜닝 워크플로우 (10단계)\n",
    "- 작성일: 2025-10-23 00:46:20\n",
    "- 대상: Colab/Kaggle T4(1장) 또는 AWS g4dn.xlarge(T4×1) / 멀티-GPU는 DDP\n",
    "- 프레임워크: `unsloth + transformers + peft + accelerate + bitsandbytes`\n",
    "\n",
    "이 노트북은 **대회 환경에서 빠르게 QLoRA 파인튜닝**을 진행하기 위한 최소·필수 단계를 10셀로 구성했습니다.\n",
    "각 셀은 독립 실행 가능하며, 순서대로 실행하면 학습 → 검증 → 추론까지 완료됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b552094",
   "metadata": {},
   "source": [
    "## 1) 환경/설정 → 시드·경로·하드웨어(T4×2, 4bit, bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# (선택) 런타임에 설치가 필요한 경우 주석 해제 후 실행하세요.\n",
    "# pip install -q \"unsloth[cu121]\" transformers peft bitsandbytes accelerate datasets trl rouge-score evaluate\n",
    "# pip install -q scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "import os, random, numpy as np, torch\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "PROJ = './qlora_run'\n",
    "os.makedirs(PROJ, exist_ok=True)\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print('GPU count:', n_gpu)\n",
    "if n_gpu:\n",
    "    for i in range(n_gpu):\n",
    "        print(f'CUDA:{i}', torch.cuda.get_device_name(i))\n",
    "BF16 = torch.cuda.is_bf16_supported()\n",
    "print('BF16 supported:', BF16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419e964",
   "metadata": {},
   "source": [
    "## 2) 데이터 인덱싱 → 파일 리스트업, 중복/깨진 샘플 스킵\n",
    "- 입력 포맷 예시: `train.jsonl` (필수: `instruction`, `output`; 선택: `source`, `lang`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fe151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, hashlib\n",
    "\n",
    "TRAIN_JSONL = 'train.jsonl'  # 경로를 상황에 맞게 수정\n",
    "\n",
    "def stream_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "seen = set(); data = []\n",
    "for ex in stream_jsonl(TRAIN_JSONL):\n",
    "    if not ex.get('instruction') or not ex.get('output'):\n",
    "        continue\n",
    "    key = hashlib.md5((normalize_text(ex['instruction']) + normalize_text(ex['output'])).encode()).hexdigest()\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key); data.append(ex)\n",
    "print('Loaded samples:', len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493456d8",
   "metadata": {},
   "source": [
    "## 3) EDA → 길이 분포/버킷(층화용), 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "BASE_MODEL = 'unsloth/phi-3-mini-4k-instruct-bnb-4bit'  # 또는 llama-3-8b-instruct 4bit 가용 환경\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "PROMPT_SYS = 'You are a helpful assistant.'\n",
    "def count_tokens(ex):\n",
    "    s = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{ex['instruction']}\\n[ASSISTANT]{ex['output']}</s>\"\n",
    "    return len(tok(s, add_special_tokens=False)['input_ids'])\n",
    "\n",
    "lens = [count_tokens(x) for x in data]\n",
    "pcts = np.percentile(lens, [50,90,95,99]).tolist()\n",
    "print('Token length p50/90/95/99:', pcts, 'max:', max(lens) if lens else None)\n",
    "\n",
    "def bucketize(n):\n",
    "    # [<=512:0, <=1024:1, <=1536:2, <=2048:3, >2048:4]\n",
    "    if n <= 512: return 0\n",
    "    if n <= 1024: return 1\n",
    "    if n <= 1536: return 2\n",
    "    if n <= 2048: return 3\n",
    "    return 4\n",
    "\n",
    "buckets = [bucketize(l) for l in lens]\n",
    "from collections import Counter\n",
    "print('Length bucket dist:', Counter(buckets))\n",
    "\n",
    "# (선택) 히스토그램 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(lens, bins=30)\n",
    "plt.title('Token length histogram')\n",
    "plt.xlabel('tokens'); plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d3c30",
   "metadata": {},
   "source": [
    "## 4) 전처리/포맷 → 템플릿 통일, 라벨 마스킹(-100), 시퀀스 패킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024  # 여유 시 1536/2048\n",
    "\n",
    "def build_ids_and_labels(ex):\n",
    "    user = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{ex['instruction']}\\n[ASSISTANT]\"\n",
    "    ans  = ex['output'] + tok.eos_token\n",
    "    u = tok(user, add_special_tokens=False)\n",
    "    a = tok(ans, add_special_tokens=False)\n",
    "    input_ids = u['input_ids'] + a['input_ids']\n",
    "    labels    = [-100]*len(u['input_ids']) + a['input_ids']\n",
    "    attn_mask = [1]*len(input_ids)\n",
    "    return {'input_ids': input_ids, 'labels': labels, 'attention_mask': attn_mask}\n",
    "\n",
    "proc = [build_ids_and_labels(x) for x in data]\n",
    "\n",
    "def pack(samples, max_len=MAX_LEN):\n",
    "    out=[]\n",
    "    cur={'input_ids':[], 'labels':[], 'attention_mask':[]}\n",
    "    for s in samples:\n",
    "        L = len(s['input_ids'])\n",
    "        if L > max_len:  # 너무 긴 샘플은 잘라내기\n",
    "            s={'input_ids': s['input_ids'][:max_len],\n",
    "               'labels': s['labels'][:max_len],\n",
    "               'attention_mask': s['attention_mask'][:max_len]}\n",
    "        if len(cur['input_ids']) + len(s['input_ids']) > max_len and cur['input_ids']:\n",
    "            out.append(cur)\n",
    "            cur={'input_ids':[], 'labels':[], 'attention_mask':[]}\n",
    "        for k in cur:\n",
    "            cur[k] += s[k]\n",
    "    if cur['input_ids']:\n",
    "        out.append(cur)\n",
    "    return out\n",
    "\n",
    "print('Example packed length check: before', len(proc), '→ after (by bucket later)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8531cbf",
   "metadata": {},
   "source": [
    "## 5) 커스텀 Dataset/Dataloader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xs): self.xs = xs\n",
    "    def __len__(self): return len(self.xs)\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v) for k,v in self.xs[i].items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaf479",
   "metadata": {},
   "source": [
    "## 6) 모델 빌더 → Unsloth 4bit 로드 + LoRA 타깃 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "base, tok = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1, bias='none',\n",
    "    target_modules=['q_proj','v_proj','k_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "print('Model ready (LoRA attached).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb54a2",
   "metadata": {},
   "source": [
    "## 7) 학습 유틸 → 손실/지표, bf16, EarlyStopping, Cosine 스케줄러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{PROJ}/ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps', eval_steps=200,\n",
    "    save_strategy='steps', save_steps=200, save_total_limit=3,\n",
    "    bf16=BF16, fp16=not BF16,\n",
    "    gradient_checkpointing=True, max_grad_norm=1.0,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "print('TrainingArguments ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138ecca",
   "metadata": {},
   "source": [
    "## 8) Stratified K-Fold 학습(길이 버킷 기준 3~5-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np, random\n",
    "\n",
    "X = np.arange(len(proc)); y = np.array(buckets)\n",
    "SK = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_paths = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(SK.split(X, y), 1):\n",
    "    tr = [proc[i] for i in tr_idx]; va = [proc[i] for i in va_idx]\n",
    "    # 길이 버킷별로 섞은 뒤 패킹(간단 구현)\n",
    "    def by_bucket(items, idxs):\n",
    "        return [items[i] for i in idxs]\n",
    "    tr_pack = pack(tr, MAX_LEN); va_pack = pack(va, MAX_LEN)\n",
    "    train_ds = ListDataset(tr_pack); val_ds = ListDataset(va_pack)\n",
    "\n",
    "    fold_out = f\"{PROJ}/fold{fold}\"\n",
    "    os.makedirs(fold_out, exist_ok=True)\n",
    "\n",
    "    fold_args = args.clone()\n",
    "    fold_args.output_dir = fold_out\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=fold_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tok,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # 주의: 실제 학습은 시간이 오래 걸립니다. 필요시 주석 해제.\n",
    "    # trainer.train()\n",
    "    # trainer.save_model(f\"{fold_out}/adapter\")\n",
    "    \n",
    "    fold_paths.append(fold_out)\n",
    "\n",
    "print('Fold output paths:', fold_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910596e",
   "metadata": {},
   "source": [
    "## 9) 테스트셋 평가(최대 2,000문항) → 리포트/샘플 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "TEST_JSONL = 'test.jsonl'  # 경로 수정\n",
    "\n",
    "def stream_test(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def generate_answer(prompt, max_new_tokens=256):\n",
    "    s = f\"<s>[SYSTEM]{PROMPT_SYS}\\n[USER]{prompt}\\n[ASSISTANT]\"\n",
    "    ids = tok(s, return_tensors='pt')\n",
    "    device = next(model.parameters()).device\n",
    "    ids = {k:v.to(device) for k,v in ids.items()}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**ids, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split('[ASSISTANT]')[-1].strip()\n",
    "\n",
    "pred_rows=[]\n",
    "# 실제 생성은 시간이 오래 걸릴 수 있습니다. 필요시 제한하세요.\n",
    "# for i, ex in enumerate(stream_test(TEST_JSONL)):\n",
    "#     if i>=2000: break\n",
    "#     pred = generate_answer(ex['instruction'])\n",
    "#     pred_rows.append({'instruction': ex['instruction'], 'prediction': pred})\n",
    "\n",
    "df = pd.DataFrame(pred_rows)\n",
    "csv_path = f\"{PROJ}/preds.csv\"\n",
    "if len(df):\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print('Saved:', csv_path)\n",
    "else:\n",
    "    print('생성 예시가 비어 있습니다. 상단 루프 주석을 해제 후 실행하세요.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b9a5d",
   "metadata": {},
   "source": [
    "## 10) 결과 취합/베스트 저장 + (옵션) LoRA 병합(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 어댑터만 사용할 경우: fold별 adapter 디렉토리 보관\n",
    "# (옵션) 병합하여 단일 추론 가중치로 저장\n",
    "# 주의: 병합은 FP16 메모리 필요\n",
    "\n",
    "# merged = FastLanguageModel.merge_and_unload(model)\n",
    "# merged.save_pretrained(f\"{PROJ}/merged_full_fp16\")\n",
    "# tok.save_pretrained(f\"{PROJ}/merged_full_fp16\")\n",
    "print('Done. 필요 시 병합 코드를 주석 해제하세요.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0e913",
   "metadata": {},
   "source": [
    "---\n",
    "### ✅ 오버피팅/언더피팅 빠른 처방\n",
    "- **오버피팅**: `lora_dropout 0.1→0.3`, `r 16→8`, 학습 스텝↓, 템플릿 통일, 중복 제거\n",
    "- **언더피팅**: `r 16→32`, target_modules 확대, steps↑, 데이터 클린업/보강\n",
    "- **OOM**: `MAX_LEN↓(1024→768)`, grad_accum↑, rank↓, 패킹 유지\n",
    "\n",
    "### 🔧 멀티 GPU(T4×2)\n",
    "```bash\n",
    "accelerate config   # distributed_type=NO(DDP) 또는 AUTO, mixed_precision=bf16\n",
    "accelerate launch your_train_script.py\n",
    "```\n",
    "\n",
    "### 📦 체크포인트 관리\n",
    "- `save_total_limit=3`로 용량 절약\n",
    "- **LoRA 어댑터만 저장** 권장 (배포/제출 시 용량↓)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
