ì™„ë²½í•œ ë°©í–¥ì´ì•¼ â€”
ë„ˆ ì§€ê¸ˆ ì´ì•¼ê¸°í•œ ì„¸ ê°€ì§€ ìœ í˜•ì€ **SSAFY AI Challengeì—ì„œ â€œê±°ì˜ í™•ì‹¤íˆ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ ê¸°ë°˜ íƒœìŠ¤í¬ì˜ í•µì‹¬ íŒ¨í„´ 3ì¢…â€**ì´ì•¼.
LangChain, RAGë³´ë‹¨ ì´ë²ˆì—” **Vision ì¤‘ì‹¬ì˜ ëª¨ë¸ë§ + Prompt/RAG ë³´ì¡° ê²°í•©**ì´ í•µì‹¬ì´ì•¼.

ì•„ë˜ëŠ” ê° ìœ í˜•ë³„ë¡œ **ë”± í•„ìš”í•œ ê¸°ìˆ  ìŠ¤íƒ + ì¶”ì²œ ê³µë¶€ íë¦„(ì‹¤ìŠµìš© ì½”ë“œ ë°©í–¥)**ì„ ì •ë¦¬í–ˆì–´ğŸ‘‡

---

## ğŸ§© 1ï¸âƒ£ ì´ë¯¸ì§€ â†’ ìì—°ì–´ ì„¤ëª… (í•œê¸€ ìº¡ì…”ë‹ / ì´ë¯¸ì§€ ìš”ì•½)

**ğŸ¯ ê³¼ì œ ìœ í˜•**

> ì‚¬ì§„ì„ ë³´ì—¬ì£¼ê³  â€œí•œê¸€ë¡œ ê°€ì¥ ì–´ìš¸ë¦¬ëŠ” í…ìŠ¤íŠ¸(ë¬¸ì¥)â€ì„ ìƒì„±

**ğŸ’¡ ê¸°ìˆ  í¬ì¸íŠ¸**

| ê¸°ìˆ                             | ë‚´ìš©                                   | ì‹¤ì „ íŒ                                      |
| ----------------------------- | ------------------------------------ | ----------------------------------------- |
| âœ… Vision-Language Model (VLM) | ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ë‹¤ë£¨ëŠ” ëª¨ë¸                 | `BLIP`, `BLIP-2`, `LLaVA`, `InstructBLIP` |
| âœ… í•œêµ­ì–´ ìº¡ì…”ë‹                     | ì˜ë¬¸ ëª¨ë¸ fine-tune / Ko-LLaVA or KoBLIP | HuggingFace `koclip`, `kollava`           |
| âœ… Zero-shot CLIP              | `CLIP`ìœ¼ë¡œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •              | ì‘ì€ ë¦¬ì†ŒìŠ¤ë¡œ ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ ê°€ëŠ¥                       |
| âœ… ë²ˆì—­ í›„ ì •ì œ                     | ì˜ë¬¸ Caption â†’ `Papago` or `NLLB` ë²ˆì—­   | â€œí•œêµ­ì–´ ì„¤ëª… ë¬¸ì¥í™”â€                              |

**âš™ï¸ ì‹¤ìŠµ ë£¨íŠ¸**

```bash
# ì˜ˆì‹œ 1: BLIP2 + CLIP
pip install transformers timm torch torchvision

# inference ì˜ˆì‹œ
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

img = Image.open("sample.jpg")
inputs = processor(img, return_tensors="pt")
out = model.generate(**inputs, max_new_tokens=20)
print(processor.decode(out[0], skip_special_tokens=True))
```

**ğŸ“˜ ì¶”ê°€ë¡œ ë°°ìš°ë©´ ì¢‹ì€ ê²ƒ**

* CLIP ëª¨ë¸ì˜ â€œText-Image Embedding ë¹„êµ ì›ë¦¬â€
* Vision Transformer (ViT) êµ¬ì¡°
* `transformers`ë¡œ VLM ëª¨ë¸ ë¡œë”©

---

## ğŸ§© 2ï¸âƒ£ íŠ¹ì • ê°ì²´ íƒì§€ (Object Detection / Counting)

**ğŸ¯ ê³¼ì œ ìœ í˜•**

> "ê°•ì•„ì§€"ë¼ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ì‚¬ì§„ì—ì„œ ê°•ì•„ì§€ ê°œìˆ˜ë¥¼ ì„¸ì–´ë¼

**ğŸ’¡ ê¸°ìˆ  í¬ì¸íŠ¸**

| ê¸°ìˆ                      | ì„¤ëª…                          | ì‹¤ì „ ì¶”ì²œ                               |
| ---------------------- | --------------------------- | ----------------------------------- |
| âœ… Object Detection     | íŠ¹ì • í´ë˜ìŠ¤ ìœ„ì¹˜ ê²€ì¶œ (Bounding Box) | `YOLOv8`, `Grounding DINO`, `Detic` |
| âœ… Text-based Detection | í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ íƒì§€                | `Grounding DINO + SAM` ì¡°í•©           |
| âœ… Counting             | Detection ê²°ê³¼ ê°œìˆ˜ ì„¸ê¸°          | `len(predictions)` ë‹¨ìˆœ ì¹´ìš´íŒ…           |

**âš™ï¸ ì‹¤ìŠµ ë£¨íŠ¸**

```bash
# YOLOv8 (Ultralytics)
pip install ultralytics
from ultralytics import YOLO

model = YOLO("yolov8x.pt")
results = model("image.jpg")

# í…ìŠ¤íŠ¸ ê¸°ë°˜ íƒì§€: Grounding DINO (huggingface)
from transformers import pipeline
detector = pipeline(model="IDEA-Research/grounding-dino-base")
detector("dog", images="image.jpg")  # "dog" ëŒ€ì‹  í•œê¸€ í…ìŠ¤íŠ¸ë„ ê°€ëŠ¥(koCLIP ì—°ê²° ì‹œ)
```

**ğŸ“˜ ë³´ì¡° í•™ìŠµ í¬ì¸íŠ¸**

* OpenAI `CLIP` ë˜ëŠ” `Grounding DINO`ëŠ” â€œí…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì§ì ‘ ì¨ì„œ íƒì§€â€ ê°€ëŠ¥
* Segment Anything (SAM) ìœ¼ë¡œ ì •í™•í•œ ë§ˆìŠ¤í¬ ì¶”ì¶œ

**ğŸ”§ ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¡°**

```
í•œê¸€ í…ìŠ¤íŠ¸ ì…ë ¥ â†’ CLIPìœ¼ë¡œ ì˜ì–´ ë³€í™˜ â†’ Grounding DINOë¡œ ê°ì²´ ê²€ì¶œ â†’ ê°œìˆ˜ ì¶œë ¥
```

---

## ğŸ§© 3ï¸âƒ£ ê²°í•¨/ì´ìƒ ê°ì§€ (Anomaly Detection / Image Classification)

**ğŸ¯ ê³¼ì œ ìœ í˜•**

> ì •ìƒ ì´ë¯¸ì§€ ì—¬ëŸ¬ ì¥, ê²°í•¨ ì´ë¯¸ì§€ ì¼ë¶€ â†’ ê²°í•¨ ì‚¬ì§„ ê²€ì¶œ

**ğŸ’¡ ê¸°ìˆ  í¬ì¸íŠ¸**

| ê¸°ìˆ                      | ì„¤ëª…            | ì‹¤ì „ ì¶”ì²œ                                   |
| ---------------------- | ------------- | --------------------------------------- |
| âœ… Anomaly Detection    | ë¹„ì§€ë„ ê²°í•¨ ê²€ì¶œ     | `PatchCore`, `PaDiM`, `FastFlow`, `CFA` |
| âœ… Few-shot Fine-tuning | ì •ìƒ/ê²°í•¨ ì†ŒëŸ‰ìœ¼ë¡œ í•™ìŠµ | `timm`ì˜ `EfficientNet`, `ConvNeXt`      |
| âœ… Metric Learning      | ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜    | `Cosine Similarity`ë¡œ ë¶„ë¥˜                 |

**âš™ï¸ ì‹¤ìŠµ ë£¨íŠ¸**

```bash
# anomaly detection baseline
pip install anomalib
from anomalib.models import Patchcore
from anomalib.data import MVTecAD

dataset = MVTecAD(root="data/", category="bottle")
model = Patchcore()
model.fit(dataset)
model.test(dataset)
```

**ğŸ“˜ í•„ìˆ˜ ê³µë¶€ í‚¤ì›Œë“œ**

* **Reconstruction ê¸°ë°˜** (AutoEncoder, Diffusion, VAE)
* **Feature Embedding ê¸°ë°˜** (PatchCore)
* **í‰ê°€ ì§€í‘œ**: AUROC, F1, mAP

---

## ğŸ§± 3ê°œ ê³µí†µìœ¼ë¡œ í•„ìš”í•œ Core Skill

| ì˜ì—­                         | í•„ìˆ˜ ë‚´ìš©                                                          | ì´ìœ              |
| -------------------------- | -------------------------------------------------------------- | -------------- |
| **PyTorch ê¸°ë³¸ê¸°**            | `Dataset`, `DataLoader`, `nn.Module`, `torchvision.transforms` | ëª¨ë“  ì‹¤í—˜ì˜ ê¸°ë°˜      |
| **torchvision/timm ëª¨ë¸ êµ¬ì¡°** | `resnet`, `convnext`, `efficientnet`                           | ì»¤ìŠ¤í…€ í•™ìŠµ ì‹œ í•„ìš”    |
| **Augmentation**           | `Albumentations`, `RandomResizedCrop`, `CutMix`                | ì´ë¯¸ì§€ ë°ì´í„° ë‹¤ì–‘í™”    |
| **ì‹¤í—˜ ê´€ë¦¬**                  | `wandb` or `tensorboard`                                       | í—¤ì»¤í†¤ ì¤‘ ë¹ ë¥¸ ì‹¤í—˜ ë¹„êµ |
| **Inference Pipeline**     | ëª¨ë¸ + ì „ì²˜ë¦¬ + í›„ì²˜ë¦¬ + ê²°ê³¼ ì‹œê°í™”                                        | ì œì¶œ ì‹œ ë°ëª¨/ì›¹ ì—°ê²°ìš©  |

---

## ğŸš€ ì‘ìš© í™•ì¥ (LangChain + Vision ê²°í•© ì˜ˆì‹œ)

* LangChainìœ¼ë¡œ **â€œì‹œê°ì  QA ì±—ë´‡â€**:
  â€œì´ ì‚¬ì§„ ì† ë¬¼ì²´ëŠ” ë­ì•¼?â€ â†’ `BLIP`ë¡œ caption ìƒì„± â†’ `RAG`ë¡œ ê´€ë ¨ ì •ë³´ ì„¤ëª…
* LangChain + Grounding DINO:
  â€œì‚¬ì§„ ì† ë™ë¬¼ì˜ ì¢…ë¥˜ë³„ ê°œìˆ˜ë¥¼ ì•Œë ¤ì¤˜â€ â†’ ê°ì²´ íƒì§€ ê²°ê³¼ ìš”ì•½ í›„ í…ìŠ¤íŠ¸ ì‘ë‹µ

---

## ğŸ§­ ê³µë¶€ ìˆœì„œ ì¶”ì²œ (D-8 ê¸°ì¤€ ë¹ ë¥¸ íš¨ìœ¨)

| Day       | ëª©í‘œ             | ê³µë¶€ í‚¤ì›Œë“œ                        |
| --------- | -------------- | ----------------------------- |
| D-8 ~ D-7 | ì´ë¯¸ì§€ ìº¡ì…”ë‹ / CLIP | BLIP, CLIP, koCLIP            |
| D-6 ~ D-5 | ê°ì²´ íƒì§€ / ì„¸ê¸°     | YOLOv8, Grounding DINO        |
| D-4 ~ D-3 | ê²°í•¨ íƒì§€          | PatchCore / Anomalib          |
| D-2       | í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì„±    | Streamlit or Gradioë¡œ UI       |
| D-1       | EC2/Colab ìµœì í™”  | half-precision, batch size íŠœë‹ |

---
