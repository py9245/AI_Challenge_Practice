####################
Cell 2
import os
import platform
import torch

print(f"Python: {platform.python_version()}")
print(f"Torch: {torch.__version__}")
print(f"CUDA devices: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    for idx in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(idx)
        print(f"  GPU {idx}: {props.name} ({props.total_memory / 1024 ** 3:.1f} GB)")
else:
    print("CUDA not available; switch Kaggle accelerator to GPU (T4x2).")

!nvidia-smi
!df -h /kaggle/working

####################
Cell 4
%%capture
!pip install -q -U transformers accelerate einops tiktoken huggingface_hub qwen-vl-utils

####################
Cell 6
import json
import math
import os
import random
import re
from collections import Counter
from pathlib import Path

import numpy as np
import pandas as pd
from IPython.display import display
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

import matplotlib.pyplot as plt
import torch
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoProcessor

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

plt.rcParams["figure.figsize"] = (6, 6)
plt.rcParams["axes.grid"] = False
pd.set_option("display.max_columns", None)

os.environ["TOKENIZERS_PARALLELISM"] = "false"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

INPUT_DIR = Path("/kaggle/input")
WORK_DIR = Path("/kaggle/working")
ARTIFACT_DIR = WORK_DIR / "artifacts"
ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)

# ==============================
# Hugging Face Token Load
# ==============================
from kaggle_secrets import UserSecretsClient

user_secrets = UserSecretsClient()
HF_TOKEN = user_secrets.get_secret("HF_TOKEN")

if HF_TOKEN:
    os.environ["HF_TOKEN"] = HF_TOKEN
    print("✅ HF_TOKEN successfully loaded!")
else:
    print("⚠️ HF_TOKEN not found. Add it via Kaggle Secrets before downloading gated models.")

####################
Cell 8
import zipfile

data_keyword = "2025-ssafy-14"
archive_candidates = sorted(INPUT_DIR.rglob(f"{data_keyword}.zip"))
if archive_candidates:
    archive_path = archive_candidates[0]
    DATA_ROOT = WORK_DIR / data_keyword
    if not (DATA_ROOT / "train.csv").exists():
        print(f"Extracting {archive_path} -> {DATA_ROOT}")
        DATA_ROOT.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(archive_path, "r") as zf:
            zf.extractall(DATA_ROOT)
    else:
        print(f"Using existing extracted dataset at {DATA_ROOT}")
else:
    print("Zip archive not located; searching for raw CSV files in /kaggle/input ...")
    csv_candidates = sorted(INPUT_DIR.rglob("train.csv"))
    if not csv_candidates:
        raise FileNotFoundError("Unable to locate train.csv under /kaggle/input")
    DATA_ROOT = csv_candidates[0].parent
    print(f"Using dataset directory detected at {DATA_ROOT}")

print(f"DATA_ROOT resolved to: {DATA_ROOT.resolve()}")
print("Contents:")
for child in sorted(DATA_ROOT.iterdir()):
    print(f"- {child.name}")

####################
Cell 10
train_df = pd.read_csv(DATA_ROOT / "train.csv")
test_df = pd.read_csv(DATA_ROOT / "test.csv")
sample_submission = pd.read_csv(DATA_ROOT / "sample_submission.csv")

train_df["answer"] = train_df["answer"].str.lower()
train_df["image_path"] = train_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))
test_df["image_path"] = test_df["path"].apply(lambda p: str((DATA_ROOT / p).resolve()))

print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(f"Sample submission shape: {sample_submission.shape}")

display(train_df.head())

####################
Cell 12
label_distribution = train_df["answer"].value_counts().sort_index()
print("Label distribution (train):")
display(label_distribution.to_frame(name="count"))

duplicate_ids = train_df["id"].duplicated().sum()
print(f"Duplicate train IDs: {duplicate_ids}")

null_summary = train_df.isna().sum()
print("Null counts (train):")
display(null_summary[null_summary > 0])

sample_paths = train_df["image_path"].sample(n=min(256, len(train_df)), random_state=SEED)
widths, heights = [], []
for path in sample_paths:
    with Image.open(path) as img:
        w, h = img.size
    widths.append(w)
    heights.append(h)

print(
    f"Sample width px -> mean: {np.mean(widths):.1f}, min: {min(widths)}, max: {max(widths)}"
)
print(
    f"Sample height px -> mean: {np.mean(heights):.1f}, min: {min(heights)}, max: {max(heights)}"
)

####################
Cell 14
num_examples = 4
fig, axes = plt.subplots(1, num_examples, figsize=(4 * num_examples, 4))
for ax, (_, row) in zip(axes, train_df.sample(num_examples, random_state=SEED).iterrows()):
    with Image.open(row["image_path"]) as img:
        ax.imshow(img)
    ax.axis("off")
    title_lines = [
        row["id"],
        row["question"][:40] + ("..." if len(row["question"]) > 40 else ""),
        f"Answer: {row['answer'].upper()}"
    ]
    ax.set_title("".join(title_lines), fontsize=9)
plt.tight_layout()

sample_row = train_df.sample(1, random_state=SEED).iloc[0]
print("Sample question:", sample_row["question"])
for key in ["a", "b", "c", "d"]:
    print(f"{key.upper()}: {sample_row[key]}")

####################
Cell 16
train_split, val_split = train_test_split(
    train_df,
    test_size=0.1,
    stratify=train_df["answer"],
    random_state=SEED,
)

train_split = train_split.reset_index(drop=True)
val_split = val_split.reset_index(drop=True)

print(f"Training rows: {len(train_split)}")
print(f"Validation rows: {len(val_split)}")
print("Validation label balance:")
display(val_split["answer"].value_counts().sort_index())

####################
Cell 18
SYSTEM_PROMPT = (
    "You are a Korean visual question answering assistant. "
    "Inspect the provided image carefully and choose the correct option among a, b, c, d. "
    "Respond strictly using a JSON object like {\"answer\": \"a\"}. No explanations, no additional text."
)

CHOICE_KEYS = ["a", "b", "c", "d"]

def create_user_block(row):
    option_lines = "".join(f"{key.upper()}. {row[key]}" for key in CHOICE_KEYS)
    return (f"질문:{row['question']}"f"선택지:{option_lines}"
        "응답 형식: JSON 예시 {\"answer\": \"a\"} (소문자, 따옴표 유지, 설명 금지)."
    )

def build_messages(row):
    return [
        {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": create_user_block(row)},
            ],
        },
    ]


def load_image(path: str) -> Image.Image:
    with Image.open(path) as img:
        return img.convert("RGB")

print("Prompt helpers ready.")

####################
Cell 20
import importlib.util
from transformers import AutoModelForVision2Seq, AutoProcessor

MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

# 로그인은 그대로
if HF_TOKEN:
    login(token=HF_TOKEN, add_to_git_credential=False)
else:
    print("Proceeding without HF login; ensure the model is public for your account.")

processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)

# torch_dtype -> dtype (deprec 경고 제거)
model_kwargs = dict(
    dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

# --- 핵심 수정: flash_attn 유무 체크 + ImportError 캐치 ---
has_fa2 = importlib.util.find_spec("flash_attn") is not None

try:
    if has_fa2:
        model = AutoModelForVision2Seq.from_pretrained(
            MODEL_ID,
            attn_implementation="flash_attention_2",
            **model_kwargs,
        )
    else:
        print("flash_attn not installed; loading without FlashAttention2 (SDPA/eager).")
        model = AutoModelForVision2Seq.from_pretrained(MODEL_ID, **model_kwargs)
except (ImportError, ValueError) as e:
    # 일부 환경에서 FA2 체크를 통과했더라도 내부에서 ImportError/ValueError가 날 수 있어 안전망으로 처리
    print(f"FlashAttention path failed ({type(e).__name__}): falling back to default attention.")
    model = AutoModelForVision2Seq.from_pretrained(MODEL_ID, **model_kwargs)

# device_map='auto'면 .to(device) 호출하지 않기 (샤딩 모델 오동작 방지)
# model.to(device)  # ← 사용하지 않음

# 생성 설정 보정
if model.generation_config.pad_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.pad_token_id = processor.tokenizer.pad_token_id
if model.generation_config.eos_token_id is None and hasattr(processor, "tokenizer"):
    model.generation_config.eos_token_id = processor.tokenizer.eos_token_id

if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "right"

model.eval()
print("Model loaded and ready.")

####################
Cell 22
LETTER_JSON_PATTERN = re.compile(r'"answer"\s*:\s*"([abcd])"')
LETTER_FALLBACK_PATTERN = re.compile(r"([abcd])")

PRED_BATCH_SIZE = 2
MAX_NEW_TOKENS = 24

def decode_answer_text(text: str) -> str:
    lowered = text.lower()
    match = LETTER_JSON_PATTERN.search(lowered)
    if match:
        return match.group(1)
    match = LETTER_FALLBACK_PATTERN.search(lowered)
    if match:
        return match.group(1)
    return "a"


def run_inference(records, batch_size: int = PRED_BATCH_SIZE, keep_raw: bool = False):
    outputs = []
    raw_outputs = [] if keep_raw else None
    for start in tqdm(range(0, len(records), batch_size)):
        batch = records[start : start + batch_size]
        prompts = []
        images = []
        for row in batch:
            prompts.append(
                processor.apply_chat_template(
                    build_messages(row),
                    tokenize=False,
                    add_generation_prompt=True,
                )
            )
            images.append(load_image(row["image_path"]))
        inputs = processor(
            text=prompts,
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        with torch.inference_mode():
            generated = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.0,
                do_sample=False,
            )
        decoded = processor.batch_decode(generated, skip_special_tokens=True)
        for response in decoded:
            answer = decode_answer_text(response)
            outputs.append(answer)
            if keep_raw:
                raw_outputs.append(response)
    if keep_raw:
        return outputs, raw_outputs
    return outputs

print("Inference helpers initialised.")

####################
Cell 24
# 12. Validation Run (OOM-safe, Kaggle T4 대응)
import gc
from tqdm.auto import tqdm
import torch

# Qwen2.5-VL 은 decoder-only 구조 → 반드시 left padding
if hasattr(processor, "tokenizer"):
    processor.tokenizer.padding_side = "left"

# 너무 크게 돌리면 T4(16GB)에서 바로 죽음 → 샘플 수 줄이고, 배치=1로 순차 처리
VALIDATION_LIMIT = min(32, len(val_split))  # 기존 128 -> 32 (필요시 16/24로 더 줄여도 됨)
validation_subset = val_split.sample(VALIDATION_LIMIT, random_state=SEED).reset_index(drop=True)
validation_records = validation_subset.to_dict("records")

val_preds, raw_responses = [], []

def _infer_one(row):
    # 이미지/프롬프트 준비
    img_path = row["image_path"] if "image_path" in row else row["image"]  # 노트북 컬럼명에 맞춰 자동 대응
    image = load_image(img_path)
    messages = build_messages(row)
    chat_text = processor.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    # 토크나이저+이미지 인코딩 (배치=1)
    inputs = processor(
        images=[image],
        text=[chat_text],
        return_tensors="pt",
        padding=True,
    ).to(model.device)

    # 실제 생성: 배치=1, no temperature, 짧게, use_cache=False 로 VRAM 절감
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=8,        # 너무 길면 메모리 팍 늘어남
            do_sample=False,         # temperature 사용 안 함 (경고 제거)
            use_cache=False          # 캐시 끄면 느리지만 VRAM 사용량↓
        )
    text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]

    # 정리
    del inputs, output_ids, image
    gc.collect(); torch.cuda.empty_cache()
    return text

for row in tqdm(validation_records, total=len(validation_records)):
    out_text = _infer_one(row)
    raw_responses.append(out_text)

    # JSON 파싱 (당신 노트북의 파서 함수명이 다르면 바꿔주세요)
    # 예: pred = parse_json_answer(out_text, choice_keys=CHOICE_KEYS, default="a")
    try:
        m = re.search(r'{"\s*answer"\s*:\s*"(a|b|c|d)"}', out_text.strip(), flags=re.I)
        pred = m.group(1).lower() if m else "a"
    except Exception:
        pred = "a"
    val_preds.append(pred)

validation_subset = validation_subset.copy()
validation_subset["prediction"] = val_preds
validation_subset["raw_response"] = raw_responses
validation_subset["is_correct"] = (validation_subset["answer"].str.lower() == validation_subset["prediction"])

val_accuracy = validation_subset["is_correct"].mean()
print(f"✅ Validation accuracy on {len(validation_subset)} samples: {val_accuracy:.4f}")

display(validation_subset.head())
display(
    validation_subset[validation_subset["is_correct"] == False][
        ["id", "answer", "prediction", "raw_response"]
    ].head()
)

####################
Cell 26
test_records = test_df.to_dict("records")

test_predictions = run_inference(test_records)
submission = pd.DataFrame({
    "id": test_df["id"],
    "answer": test_predictions,
})

submission_path = WORK_DIR / "submission.csv"
submission.to_csv(submission_path, index=False)
print(f"Saved submission to {submission_path}")

display(submission.head())

